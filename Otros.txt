                                                                                                                   Nodejs

   
- fs en Nodejs: fs es la abreviación utilizada para FileSystem, el cual, como indica el nombre, es un sistema de manejador de archivos que nos proporcionará node para poder crear, leer, actualizar o eliminar un archivo, sin tener que hacerlo 
     nosotros desde cero. Así, crear un archivo con contenido será tan fácil como escribir un par de líneas de código, en lugar de tener que lidiar con los datos binarios y transformaciones complejas y de un nivel más bajo en la computadora.
     fs existe desde el momento en el que instalamos Nodejs en nuestro computador, por lo que, para utilizarlo, podemos llamarlo desde cualquier archivo que tengamos de nuestro código con la siguiente línea:
                                                                                   
                                                                                   const fs = require("fs");
                                                                                   
     De ahí en adelante todo el módulo de FileSystem estará contenido en la variable fs. Sólo debemos utilizarlo llamando sus métodos como una clase. Esto podremos hacerlo de 3 formas: síncrono, con callbacks o con promesas.                                                                                   
     Para utilzar de forma adecuada el fs, sólo utilizaremos la palabra Sync después de cada operación que queramos realizar. Las principales operaciones que podemos hacer con fs síncrono son:
       ✓ writeFileSync = Para escribir contenido en un archivo. Si el archivo no existe, lo crea. Si existe, lo sobreescribe.
       ✓ readFileSync = Para obtener el contenido de un archivo.
       ✓ appendFileSync = Para añadir contenido a un archivo. ¡No se sobreescribe!
       ✓ unlinkSync = Es el “delete” de los archivos. eliminará todo el archivo, no sólo el contenido.
       ✓ existsSync = Corrobora que un archivo exista!
       
- fs con callbacks: Funciona muy similar a las operaciones síncronas. Sólo que al final recibirán un último argumento, que como podemos intuir, debe ser un callback. Según lo vimos en las convenciones de callbacks de la clase pasada, el 
    primer argumento suele ser un error. Esto permite saber si la operación salió bien, o si salió mal. Sólo readFile maneja un segundo argumento, con el resultado de la lectura del archivo.
    Por último: el manejo por callbacks es totalmente asíncrono, así que cuidado dónde lo usas. Las principales operaciones que podemos hacer con fs con callbacks son:
      ✓ writeFile = Para escribir contenido en un archivo. Si el archivo no existe, lo crea. Si existe, lo sobreescribe. Al sólo escribir, su callback sólo maneja: (error)=>
      ✓ readFile = Para obtener el contenido de un archivo. Como pide información, su callback es de la forma: (error, resultado)=>
      ✓ appendFile = Para añadir contenido a un archivo. ¡No se sobreescribe!, al sólo ser escritura, su callback sólo maneja: (error)=>
      ✓ unlink = Es el “delete” de los archivos. eliminará todo el archivo, no sólo el contenido. Al no retornar contenido, su callback sólo es (error)=>
      
     Si queremos utilizar promesas, entonces qudarian como:
      ✓ fs.promises.writeFile
      ✓ fs.promises.readFile
      ✓ fs.promises.appendFile
      ✓ fs.promises.unlink    

- Crypto: 
- Process: 
- Child process
- Listeners: 
- Path: 
- Cluster: 
- OS: 


- Sobre los entornos: Para que un código esté listo para llegar al cliente, es necesario que pase por diferentes fases.
  Sin embargo, para que estas fases se encuentren aisladas de las otras fases (no queremos que la fase de desarrollo tenga datos de producción, o que haya datos de producción en staging), necesitaremos crear entornos específicos para estas 
  fases. Nuestras variables cambiarán según el entorno.
  El primer uso radica en que una variable cambie según el entorno donde se esté corriendo, esto permite que pueda apuntar a una base de datos prueba o a una base de datos productiva con sólo cambiar el apuntador de dónde se está corriendo.
  Otro factor importante es el factor seguridad. Con las variables de entorno podemos ocultar la información sensible de nuestro código, como credenciales, claves de acceso, tokens, secrets, etc.                                                                                                                   
                                                                                                                   
- process: Cada vez que corremos un proceso en nodejs, éste genera un objeto llamado process, el cual contiene información referente a todo lo implicado con el proceso, cosas como:
    ✓ Uso de memoria 
    ✓ Id del proceso en el sistema operativo 
    ✓ En qué sistema operativo o plataforma está corriendo
    ✓ En qué entorno está corriendo. 
    ✓ Qué argumentos tiene el entorno.

 > Algunos elementos importantes de process
    ✓ process.cwd() : Directorio actual del proceso.
    ✓ process.pid : id del proceso en el sistema
    ✓ process.MemoryUsage() :
    ✓ process.env : Accede al objeto del entorno actual
    ✓ process.argv : Muestra los argumentos pasados por CLI (Los argumentos permiten iniciar la ejecución de un programa a partir de ciertos elementos iniciales)
    ✓ process.version : Muestra la versión del proceso (node en este caso)
    ✓ process.on() : Permite setear un listener de evento. Permitirá poner a nuestro proceso principal a la escucha de algún evento para poder ejecutar alguna acción en caso de que algo ocurra. (Listeners)
      ~ process.on('exit'); //Para ejecutar un código justo antes de la finalización del proceso.
      ~ process.on("warning"); //The 'warning' event is emitted whenever Node.js emits a process warning. A process warning is similar to an error in that it describes exceptional conditions that are being brought to the user's attention.
      ~ process.on('message'); //para poder comunicarse con otro proceso      

      ~ process.on("beforeExit"); //This event is emitted when node empties its event loop and has nothing else to schedule. Normally, the node exits when there is no work scheduled, but a listener for 'beforeExit' can make asynchronous 
        calls, and cause the node to continue.
        
      ~ process.on("rejectionHandled"); //The 'rejectionHandled' event is emitted whenever a Promise has been rejected and an error handler was attached to it (using promise.catch(), for example) later than one turn of the Node.js event 
        loop. The Promise object would have previously been emitted in an 'unhandledRejection' event, but during the course of processing gained a rejection handler.

      ~ process.on('uncaughtException'); It is emitted when an uncaught JavaScript exception bubbles all the way back to the event loop. By default, Node.js handles 
        such exceptions by printing the stack trace to stderr and exiting with code 1, overriding any previously set process.exitCode. Adding a handler for the 'uncaughtException' event overrides this default behavior. Alternatively, change 
        the process.exitCode in the 'uncaughtException' handler which will result in the process exiting with the provided exit code. Otherwise, in the presence of such handler the process will exit with 0.
        Para atrapar alguna excepción que no haya sido considerada en algún catch.
        
      ~ process.on("unhandledRejection"); //The 'unhandledRejection' event is emitted whenever a Promise is rejected and no error handler is attached to the promise within a turn of the event loop. When programming with Promises, exceptions 
        are encapsulated as "rejected promises". Rejections can be caught and handled using promise.catch() and are propagated through a Promise chain. The 'unhandledRejection' event is useful for detecting and keeping track of promises that 
        were rejected whose rejections have not yet been handled.               

    ✓ process.exit() : Permite salir del proceso. Cuando ejecutamos una salida con process.exit() como argumento, podemos enviar un código que sirve como identificador para el desarrollador sobre la razón de la salida
      Hay que conocer los códigos de salida para aber cómo utilizarlos. También podemos rear nuestros propios códigos. Algunos de los códigos importantes son:
        0 : proceso finalizado normalmente. 
        1 : proceso finalizado por excepción fatal 
        5 : Error fatal del motor V8. 
        9 : Para argumentos inválidos al momento de la ejecución.
     
 > arguments (commander): Commander es una librería para el manejo de argumentos. Permite realizar funciones como:
    ✓ Convertir flags directamente en booleanos
    ✓ Limitar sólo las flags configuradas (cualquier otra impide el procesamiento del programa)
    ✓ Colocar argumentos predeterminados.
      
- Child process: Single-threaded, non-blocking performance in Node.js works great for a single process. But eventually, one process in one CPU is not going to be enough to handle the increasing workload of your application. No matter how 
  powerful your server may be, a single thread can only support a limited load. The fact that Node.js runs in a single thread does not mean that we can’t take advantage of multiple processes and, of course, multiple machines as well.
  Using multiple processes is the best way to scale a Node application. Node.js is designed for building distributed applications with many nodes. This is why it’s named Node. Scalability is baked into the platform and it’s not something you 
  start thinking about later in the lifetime of an application.
  
  We can easily spin a child process using Node’s child_process module and those child processes can easily communicate with each other with a messaging system. The child_process module enables us to access Operating System functionalities 
  by running any system command inside a, well, child process. We can control that child process input stream, and listen to its output stream. We can also control the arguments to be passed to the underlying OS command, and we can do 
  whatever we want with that command’s output. We can, for example, pipe the output of one command as the input to another as all inputs and outputs of these commands can be presented to us using Node.js streams.

  Existen casos en los que un proceso de node necesitará crear otro proceso para poder resolver una función de gran complejidad. Algunas operaciones requieren mucho procesamiento, como:
    ✓ Lectura de archivos enormes
    ✓ Consultas a bases muy complejas 
  Por lo que, para no bloquear las tareas actuales de, un servidor, por ejemplo, ocupamos separar esa tarea en otro subproceso.
  Existen diferentes formas para que un proceso de node pueda ejecutar otro proceso, hay cuatro operadores que pueden ser utilizados y manipulados de diferentes formas
    --> https://www.freecodecamp.org/news/node-js-child-processes-everything-you-need-to-know-e69498fe970a/
   
    > fork(): The fork function is a variation of the spawn function for spawning node processes. The biggest difference between spawn and fork is that a communication channel is established to the child process when using fork, so we 
      can use the send function on the forked process along with the global process object itself to exchange messages between the parent and forked processes. We do this through the EventEmitter module interface.
      Se utiliza para ejecutar scripts Node.js en procesos secundarios y comunicarse con ellos.
          
    > spawn(): The spawn function launches a command in a new process and we can use it to pass that command any arguments. For example, here’s code to spawn a new process that will execute the pwd command.
      Esta función se utiliza para ejecutar un comando en un nuevo proceso. Permite especificar el comando a ejecutar y los argumentos que se le pasarán. También proporciona una interfaz para interactuar con la entrada, salida y 
      error estándar del proceso secundario en tiempo real. 
      Se utiliza para ejecutar comandos del sistema operativo.
   
    > exec(): It is used to test for the match in a string. If there is a match this method returns the first match else it returns NULL. Esta función se utiliza para ejecutar un comando en un subproceso con una shell. Es útil cuando se 
      necesita ejecutar comandos de shell con funcionalidades como expansión de comodines.
    
    > execFile(): 



$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$        

                                                                                                                   Express

- Express js es un framework minimalista que permitirá desarrollar servidores más complejos. Éste nos facilitará:
   ✓ Utilizar diferentes rutas para las peticiones.
   ✓ Mejorar la estructura de nuestro proyecto.
   ✓ Manejar funcionalidades más complejas y utilización de middlewares. 
  
- Método GET: Método del protocolo HTTP que permite indicar al servidor que deseamos hacer na consulta de obtención de datos.       .get() 
- Metodo POST: Sirve para “crear” recursos, POST se utiliza para operaciones donde no necesitamos  obtener un recurso, sino añadir uno. Se apoya del recurso req.body, donde elbody representa la información que el cliente envía para crear.
- Método PUT: Sirve Para poder trabajar con PUT, no sólo enviamos el body en el request, sino que demás mandamos por params el id, nombre, cualquier identificador para que el servidor epa qué recurso específicamente debe ctualizar.
  Hay dos formas de actualizar un recurso: actualizar sólo los campos requeridos, o bien mandar a actualizar el objeto completo, ambas formas son válidas cuando hablamos de actualización, y dependerá del contexto.
- Método DELETE: Como bien lo indica el nombre, este método lo utilizamos cuando queremos eliminar algún recurso. Aquí no es necesario enviar nada desde el body, sin embargo, sí es importante indicar en el req.params el identificador para 
  que el servidor reconozca qué recurso debe eliminar.
    
- Metodos en express js
  > app.set(name, value); is used to assign the setting name to value. You may store any value that you want, but certain names can be used to configure the behavior of the server. 

      
- request (req): The req object represents the HTTP request and has properties for the request query string, parameters, body, HTTP headers, and so on. In this documentation and by convention, the object is always referred to as req (and 
  the HTTP response is res) but its actual name is determined by the parameters to the callback function in which you’re working. Objeto usado dentro de los servicios de express para poder realizar consultas más complejas.
  
  > req.query: Como su nombre lo indica, query refiere a las múltiples consultas que se pueden hacer a un determinado endpoint, basta conque en la url coloquemos el símbolo ? , entonces express reconocerá que hay que meter información al 
    objeto req.query para poder utilizarlo en el endpoint. Cuando buscamos algo en nuestro navegador, llamamos a un endpoint haciendo un determinado query. Se expresa como:    ?key=valor
    
  > req.params: Se utiliza cuando necesitamos obtener elementos dinámicos desde la ruta que está llamando el cliente. para poder definir un “parámetro” dentro de la ruta a trabajar, basta con colocar el símbolo de dos puntos (:) antes del 
    parámetro, de esta manera, express reconoce que queremos que ese elemento sea dinámico.
    
  > req.body:  
  
  > ¿Qué diferencia hay con params? 
    La principal diferencia que hay entre req.params y req.query, es que en req.query puedo meter la cantidad de consultas que yo así desee, ya que las queries no vienen inmersas en la ruta, sino que son un elemento aparte.
    Así, si desconozco el número de cosas que se van a consultar en mi ruta, la mejor opción es utilizar queries, mientras que, si sólo necesito un número específico y reducido de parámetros, habría que optar por params
    Al final, no hay una mejor que otra, sirven para casos diferentes e incluso podemos utilizar ambas en la misma consulta. 
    
    
- app.use(express.urlencoded({ extended: true })); It parses incoming requests with urlencoded payloads and is based on body-parser. Returns middleware that only parses urlencoded bodies and only looks at requests where the Content-Type 
  header matches the type option. This parser accepts only UTF-8 encoding of the body and supports automatic inflation of gzip and deflate encodings. permitirá que el servidor pueda interpretar mejor los datos complejos que viajen desde la 
  url, y mapearlos correctamente en el req.query. Es decir, cuando hagamos una peticion POST, cuando lleguen, que podamos recivir esos datos. Sino, llegara un objeto vacio. 

- app.use(express.json()); This is a built-in middleware function in Express. It parses incoming requests with JSON payloads and is based on body-parser.
  
- app.use(express.static('public')); Sirve para convertir una carpeta en un recurso estático. Express busca los archivos relativos al directorio estático, por lo que el nombre del directorio estático no forma parte del URL.
    > Prefijo virtual: Para crear un prefijo virtual (donde el path de acceso no existe realmente en el sistema de archivos) para los archivos servidos por express.static, debemos especificar un path de acceso de montaje para el directorio 
      estático:  app.use('/static', express.static('public'));
      Así podemos cargar los archivos que hay en el directorio public desde el prefijo /static.
      http://localhost:3000/static/hello.html
      http://localhost:3000/static/images/kitten.jpg
    
   > Path absoluto: El path que se proporciona a la función express.static es relativo al directorio desde donde inicia el proceso node. Por eso si ejecutamos la aplicación Express desde cualquier otro directorio, es más seguro utilizar el 
     path absoluto del directorio al que desea dar servicio:   app.use('/static', express.static(__dirname + '/public'))

- next(): Esta funcion representa un parametro en adicional a req y res (req, res, next) y se usa en middlewares, generalmente cuando se manejan errores. Si el middleware no presentan ningun inconveniente, la funcion next() no tendra 
  ningun parametro, es decir, no hay error que enviar. Pero en caso de haberlo, entonces se debera mandar ese error dentro de los parentesis.
        
- Router en Express: Un router en express nos permitirá separar los endpoints “comunes” en entidades separadas que fungirán como “mini aplicaciones”, las cuales tomarán peticiones que concuerden con dicho endpoint y así redireccionarse a 
  esta mini aplicación. De esta manera, nuestro código resultará más organizado, y las diferentes entidades tendrán aislado el comportamiento interno, como configuraciones, middlewares, etc.
  
- middleware: Operación intermedia que ocurre entre la petición a la base de datos y la entrega del documento o los documentos correspondientes. 
  Funciones que se colocan en medio de la ruta y el callback (req, res) que se ejecutarán antes de comenzar a procesar la petición.
  Cada vez que utilizamos un app.use estamos utilizando un middleware. Éstas son operaciones que se ejecutan de manera intermedia entre la petición del cliente, y el servicio de nuestro servidor.
  Como lo indica el nombre: “middleware” hace referencia a un intermediario, siempre se ejecuta antes de llegar al endpoint que corresponde. los middlewares se ejecutan EN ORDEN, eso quiere decir que, si algún middleware depende de que se 
  haya realizado otra operación ejecutada por un middleware previo, los coloquemos en cascada según prioridad. Podemos utilizar un middleware para:
    ✓ Dar información sobre las consultas que se están haciendo (logs) 
    ✓ Autorizar o rechazar usuarios antes de que lleguen al endpoint (seguridad) 
    ✓ Agregar o alterar información al método req antes de que llegue al endpoint (formato) 
    ✓ Redireccionar según sea necesario (router) 
    ✓ En ciertos casos, finalizar la petición sin que llegue al endpoint (seguridad)
    
- Tipos de middleware: Una aplicación Express puede utilizar los siguientes tipos de middleware:
  ✓ Middleware a nivel de aplicación
  ✓ Middleware a nivel endpoint
  ✓ Middleware a nivel del Router
  ✓ Middleware de manejo de errores
  ✓ Middleware incorporado
  ✓ Middleware de terceros
  
                                                                                                  
                                                                                                                   
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$        

                                                                                                                   MONGODB
                                                                                                                   
- Usar mongodb en la terminal: 
  1) correr .\mongod.exe  en powershell
  2) Abrir otra ventana de powershell y copiar el siguiente link --> cd 'C:\Program Files\MongoDB\Server\7.0\bin'
  3) Despues ejecutar --> .\mongos.exe                                                                                                            

- Base de datos: Una base de datos no es más que una recopilación organizada de datos. Dichos datos deben compartir algún contexto y son almacenados con poder convertirse posteriormente en información útil para utilizarse dentro de algún
  sistema. La base de datos sólo se encargará de almacenar dichos datos.
  Algunas de las cosas que podemos señalar sobre la utilidad de una base de datos son:
   ✓ Almacenamiento más seguro: Los datos que viven en una base de datos no son modificables directamente, por lo que éstos no pueden ser cambiados tan fácilmente.
   ✓ Segmentación de datos: Podemos separar los datos en “contextos”, permitiendo así tener separados los datos de interés. 
     ○ Separar clientes potenciales de clientes sólo interesados. 
     ○ Separar productos existentes de productos fuera de stock 
     ○ Separar diferentes usuarios por género, nacionalidad, plan, etc.
   ✓ Gestión sencilla una vez configurada: Una vez que hemos definido los esquemas principales de nuestra base de datos, podremos realizar operaciones sobre estos datos como:
   ✓ Filtrar
   ✓ Ordenar
   ✓ Buscar datos específicos.
   ✓ Actualizar un conjunto de datos sin afectar o tocar otros datos.

- Modelo relacional: Gestión de datos que consiste en representar éstos como tablas relacionadas con el fin de brindar una estructura de relación sólida entre ellos.
- Modelo no relacional: Gestión de datos flexible que sustituye las tablas relacionadas, por colecciones, garantizando facilidad y dinamismo en el manejo de los datos.
- Base de datos relacional: se refiere a estructura, elación, dependencia y de cambio controlado.
- Base de datos no relacional:  refiere a algo enos estructurado, con relaciones y ependencias más flexibles, y de cambios umamente rápidos.

- Inconsistencia de datos

- CRUD: es un acrónimo que hace referencia a las cuatro operaciones fundamentales de una base de datos:
  ✓ C : Create (Crear un dato, insertarlo en la base de datos)
  ✓ R : Read (Leer un dato, mostrarlo al cliente)
  ✓ U : Update (Actualizar un dato, cambiar su información interna)
  ✓ D : Delete (Eliminar un dato, removerlo de nuestra colección.
  
  
- Comandos de apoyo
  ✓ show dbs : Muestra las bases de datos existentes.
  ✓ use <db name>: Crea una nueva base de datos (en caso de no existir) y se posiciona sobre ella
  ✓ db: Muestra en qué base de datos estamos posicionado.
  ✓ show collections: Muestra todas las colecciones disponibles en la base de datos posicionada.
  ✓ db.createCollection(name): Crea una colección en la base de datos posicionada.
  ✓ db.dropDatabase(): Elimina la base de datos actual.
  ✓ db.collection.drop(): Elimina la colección de la base de datos posicionada.

- Primeros comandos CRUD: CR
  ✓ db.collection.insertOne(doc) : Agrega un nuevo documento a la colección seleccionada.
  ✓ db.collection.insertMany(docs): Agrega múltiples documentos a la colección seleccionada (dado un arreglo de documentos).
  ✓ db.collection.findOne(opt): Busca un elemento que cumpla con los criterios de búsqueda (opt), devuelve el primer documento que cumpla con dicho criterio.
  ✓ db.collection.find(opt):Devuelve todos los documentos que cumplan con dicho criterio.
  ✓ db.collection.find(opt).pretty(): Añadido para hacer más presentables los resultados de un find()    

- Conteo de datos: Los comandos de conteo para determinar el número de documentos en una colección son
  ✓ db.collection.estimatedDocumentC count() Cuenta el estimado más próximo al número de documentos según su metadata.
  ✓ db.collection.countDocuments(opt) Cuenta los documentos que cumplan con el criterio definido en las opciones (opt).
  
- opt (options): agregando opciones
  En muchas consultas encontramos el elemento (opt), esto hace referencia a las opciones de filtros de búsqueda que podemos realizar al momento de buscar un valor, la sintaxis elemental de un opt es: {propiedad:valor}
                                                                             db.users.find({gender: "M})
        
- Filtros: Las búsquedas del mundo real no siempre requieren que un valor sea igual a otro. En ocasiones necesitamos que sea menor, mayor, diferente de, entre otras cosas. Los filtros pueden agregarse dentro de los elementos de criterio 
  (opt) con ayuda del símbolo $, además, podemos agregar más de un filtro para asegurarnos que el documento se ajuste a criterios muy específicos. Entonces, la sintaxis general será:
                                                                             db.coll.find( {key: {$operator: val}} )
                                                                             
- MongoDB: Operadores para Filtros de Query
  ✓ $and: Realiza operación AND -> sintaxis: {$and: [ {},{} ] }
  ✓ $or: Realiza operación OR -> sintaxis: {$or: [ {},{} ] }
  ✓ $lt: Coincide con valores que son menores que un valor especificado.
  ✓ $lte: Coincide con valores menores o iguales a un valor especificado.
  ✓ $gt: Coincide con valores mayores a un valor especificado.
  ✓ $gte: Coincide con valores mayores o iguales a un valor especificado.
  ✓ $ne: Coincide con valores que no son iguales a un valor especificado.
  ✓ $eq: Selecciona los documentos que son iguales a un valor especificado.                                                                                        
  ✓ $exists: Selecciona los documentos según la existencia de un campo.
  ✓ $in: Selecciona los documentos especificados en un array. sintaxis: {key:{$in: [array of values] } }
  ✓ $nin: Coincide con ninguno de los valores especificados en un array.
  ✓ $size: Coincide con el número de elementos especificados.
  ✓ $all: Coincide con todos los valores definidos dentro de un array.
  ✓ $elemMatch: Coincide con algún valor definido dentro del query.                                                                                             
  
- MongoDB: Búsqueda Avanzada
  ✓ db.coll.distinct( val ) devuelve un array con los distintos valores que toma un determinado campo en los documentos de la colección.
  ✓ db.coll.find({doc.subdoc:value}) Se utiliza para filtrar subdocumentos.
  ✓ db.coll.find({name: /^Max$/i}) filtra utilizando expresiones regulares
  
- Proyecciones: En ocasiones no necesitamos toda la información de un documento. Si tenemos un documento con 100 propiedades, podemos definir sólo las propiedades que queremos obtener.
  Una proyección se incluye al momento de hacer una búsqueda, (siempre como segundo argumento) y es el equivalente a decirle a la base de datos: “sólo necesito ésto”
  Así, podríamos decir db.users.find({},{name:1}); Lo cual indica que, el campo “name” es el único que necesitamos obtener por parte del documento, ahorrándonos espacio y complejidad en el resultado
  
- Sort: Sirve para poder hacer un ordenamiento de la información. El ordenamiento se define con 1 o -1 para hacer el ordenamiento ascendente o descendente respectivamente.
  La sintaxis es: db.collection.find().sort({val_A:1,val_B:-1})
  La razón por la cual podemos agregar múltiples valores de ordenamiento, es en caso de que dos documentos tengan el mismo valor, podamos ordenarlos bajo otro criterio  
  
- Skip: Omite el número de documentos indicados:Podemos usarlo cuando hagamos paginaciones, cuando necesitemos ignorar un valor que sabemos que es innecesario, etc. Su sintaxis es: .skip(offset)
- Limit: Limita el número de documentos devueltos. De manera que podamos hacer diferentes niveles de paginación (Tu página puede devolver 5 elementos por página, o bien 100, tú decides). Su sintaxis es: .limit(num)  
  
- CRUD (update): Las operaciones Update se pueden realizar de dos maneras: Actualizar un documento, o actualizar múltiples documentos.
  ✓ db.collection.updateOne(query,update,option)
  ✓ query: sirve para filtrar qué elementos actualizar (usa los filtros iguales al find)
  ✓ update: Apartado para indicar qué actualizar de los documentos que cumplen con el filtro. Update tiene sus propios operadores como $set, $unset, $inc, $rename, $mul, $min, $max
  ✓ option: Opciones a tomar en cuenta para la actualización (como upsert, que inserta el valor en caso de que el documento a actualizar ni siquiera exista).
  ✓ db.collection.updateMany(query,update,options) Actualiza múltiples documentos que cumplan con el criterio.
  
- CRUD (Delete): Nuestra última operación es para eliminar datos, si bien hay muchas variantes de una eliminación, sólo veremos las dos principales.
✓ db.collection.deleteOne({key:val}) : Elimina sólo el primer elemento que cumpla con el criterio, se usa principalmente para encontrar identificadores específicos. Se recomienda no utilizar si somos conscientes de que el valor a buscar no 
   es repetido.
✓ db.collection.deleteMany({key:val}) : Elimina todos los documentos que cumplan con el criterio, se usa cuando sabemos que más de un valor va a contar con ese valor y necesitamos hacer una limpieza general.  


- Mongoose (population): Operación que permite transformar la referencia de un documento en su documento correspondiente en la colección indicada.
  Una population implica obtener un documento referenciado dentro de otro documento, con el fin de obtener ambos en una sola búsqueda. Consiste en almacenar el id de un documento, como propiedad de otro documento. A esto se le     
  conoce como “referencia”. Populate hace referencia a “poblar” de un id a un documento completo. (referencia a la población humana).
  Algunas cosas a considerar antes de comenzar con su uso:
    ✓ populate es un método propio de mongoose, por lo que tenemos que instalarlo.
    ✓ Hay que tener siempre claro el nombre de la propiedad dentro del objeto, así también como la referencia de la colección, para poder hacer un populate efectivo.
    ✓ Recuerda no guardar directamente el valor a referenciar en el _id, sino asignarle otro nombre (se profundizará en el ejemplo).

- Mongoose (indexing): Indexes support the efficient execution of queries in MongoDB. Without indexes, MongoDB must perform a collection scan, i.e. scan every document in a collection, to select those documents that match the query 
  statement. If an appropriate index exists for a query, MongoDB can use the index to limit the number of documents it must inspect.
  Es un recurso utilizado en MongoDB para poder hacer consultas mucho más rápidas al colocarse en una propiedad de un documento. Éste nos permitirá tener una referencia previa al momento de buscar un documento, con el fin de evitar recorrer 
  toda la colección, documento por documento, hasta encontrar dicho valor. El índice se asocia a un atributo del documento y permite que las búsquedas se hagan desde puntos específicos, evitando el recorrido completo de la colección.
  Prever un buen plan de indexación evitará problemas de lentitud en las consultas y se considera una práctica necesaria a nivel enterprise, al momento de configurar los schemas de nuestros distintos modelos
  
  Un índice no debe ser utilizado en todos los campos, sólo deben ser utilizados en los campos que sepamos tienen repercusión en nuestras búsquedas. Colocar un índice en cada campo de cada documento, significa alentar procesos de escritura 
  en cada insert, así también como generar un almacenamiento adicional e innecesario en la base de datos.
  
  > Tipos de indices: Prever un buen plan de indexación evitará problemas de lentitud en las consultas y se considera una práctica necesaria a nivel enterprise, al momento de configurar los schemas de nuestros distintos modelos.
     ✓ compound: Se utiliza cuando requerimos utilizar más de una indexación y queremos definir el orden con el cual se realiza el ordenamiento (ordenando con 1 para ascendente y -1 para descendente, igual que un sort: {campo: 1 , campo: -1}
     ✓ multikey: Se utiliza cuando requerimos hacer una indexación de valores que se encuentran de manera interna en un array.
     ✓ text: Se utiliza para poder basarse en búsquedas de palabras “específicas” con el fin de poder tomar referencia de un texto a partir de dichas palabras.
     ✓ geospatial: Se utiliza para almacenar data geoespacial de dos coordenadas, utiliza una esfera 2d para poder trabajar los datos. 

  > population: Operación que permite transformar la referencia de un documento en su documento correspondiente en la colección indicada.
    Una population implica obtener un documento referenciado dentro de otro documento, con el fin de obtener ambos en una sola búsqueda. Consiste en almacenar el id de un documento, como propiedad de otro documento. A esto se le     
    conoce como “referencia”. Populate hace referencia a “poblar” de un id a un documento completo. (referencia a la población humana).
    Algunas cosas a considerar antes de comenzar con su uso:
      ✓ populate es un método propio de mongoose, por lo que tenemos que instalarlo.
      ✓ Hay que tener siempre claro el nombre de la propiedad dentro del objeto, así también como la referencia de la colección, para poder hacer un populate efectivo.
      ✓ Recuerda no guardar directamente el valor a referenciar en el _id, sino asignarle otro nombre (se profundizará en el ejemplo)

  > Configurando una population por default: Para poder “poblar” el resultado de la operación find() del estudiante y obtener los cursos, fue necesario llamar a “populate” después de la operación. Sin embargo, tener que colocar el populate 
    puede resultar molesto si utilizamos constantemente el modelo de estudiante. Mongoose tiene la posibilidad de definir “middlewares” para sus documentos y modelos con el fin de automatizar operaciones que consideremos recurrentes. 

- Mongoose (aggregation): Consiste en la realización de múltiples operaciones, eneralmente sobre múltiples documentos. ueden utilizarse para:              https://stackoverflow.com/questions/24714166/full-text-search-with-weight-in-mongoose
    ✓ Agrupar documentos con base en un criterio específico.
    ✓ Realizar alguna operación sobre dichos documentos, con el fin de obtener un solo resultado.
    ✓ Analizar cambios de información con el paso del tiempo.

  > Funcionamiento: Los aggregation pipelines consistirán en un conjunto de pasos (stages), donde cada paso corresponderá a una operación a realizar. Podemos definir tantas stages como necesitemos con el fin de llegar a los resultados 
    esperados. Los documentos resultantes de la stage que finalice, se utilizan como “input” de la siguiente stage, y así sucesivamente hasta llegar al final. 
    Un ejemplo de un pipeline de aggregation puede ser: 
    1. Primero filtra los documentos que tengan un valor x mayor a 20 2. Luego ordénalos de mayor a menor 3. Luego en un nuevo campo devuelve el valor máximo 4. Luego en un nuevo campo devuelve el valor mínimo 5. Luego en un nuevo campo 
       devuelve la suma total de todos los documentos

  > Principales stages disponibles en un aggregation pipeline
    ✓ $count : Cuenta el número de documentos disponibles que se encuentren en la stage actual.
    ✓ $group: Permite agrupar los documentos disponibles en nuevos grupos según un criterio especificado. cada grupo cuenta con un _id nuevo, además de los valores acumulados \
    ✓ $limit: Limita el número de documentos que saldrán de dicha stage.
    ✓ $lookup: Permite realizar un “left join” (combinación de campos), de una colección de la misma base de datos a los documentos de la stage actual.
    ✓ $set / $addFields : Agregan una nueva propiedad a los documentos que se encuentren en dicha stage.
    ✓ $skip: Devuelve sólo los documentos que se encuentren después del offset indicado.
    ✓ $sort: Ordena los documentos en la stage actual.
    ✓ $match -->  Devuelve sólo los documentos que cumplan con un criterio de búsqueda, podemos colocar filtros comunes aquí. Filters the document stream to allow only matching documents to pass unmodified into the next pipeline stage. 
      It uses standard MongoDB queries. For each input document, outputs either one document (a match) or zero documents (no match).
    ✓ $group --> Groups input documents by a specified identifier expression and applies the accumulator expression(s), if specified, to each group. Consumes all input documents and outputs one document per each distinct group. The output 
       documents only contain the identifier field and, if specified, accumulated fields.
    ✓ $sum --> Returns a sum of numerical values. Ignores non-numeric values.
    ✓ $first --> Returns the result of an expression for the first document in a group of documents. Only meaningful when documents are in a defined order.
    ✓ $sort ---> Reorders the document stream by a specified sort key. Only the order changes; the documents remain unmodified. For each input document, outputs one document.
    ✓ $push --> returns an array of all values that result from applying an expression to documents.
    ✓ $$ROOT --> significa los resultados del stage anterior
    ✓ $project --> Reshapes each document in the stream, such as by adding new fields or removing existing fields. For each input document, outputs one document. Takes a document that can specify the inclusion of fields, the suppression of 
       the _id field, the addition of new fields, and the resetting of the values of existing fields. Alternatively, you may specify the exclusion of fields.
    ✓ $merge --> Escribe los resultados del pipeline en una colección. Debe ser la última stage del pipeline para poder funcionar.
       Writes the resulting documents of the aggregation pipeline to a collection. The stage can incorporate (insert new documents, merge documents, replace documents, keep existing documents, fail the operation, process documents with a 
       custom update pipeline) the results into an output collection. IT MUST BE USED BY USING A $project BEFORE.    
    
    
- Mongoose (Paginate): Pagination is the process of separating print or digital content into discrete pages. For print documents and some online content, pagination also refers to the automated process of adding consecutive numbers to 
  identify the sequential order of pages.                                  https://stackoverflow.com/questions/5539955/how-to-paginate-with-mongoose-in-node-js
  
  > mongoose-paginate-v2 is a pagination library having a page wrapper. The main usage of the plugin is you can alter the return value keys directly in the query itself so that you don't need any extra code for transformation.                                    

  > Model.paginate([filter], [options], [callback])
    ✓ docs: Los documentos devueltos en la página
    ✓ totalDocs: Los documentos totales de la consulta realizada.
    ✓ limit: Límite de resultados por página.
    ✓ page: Página actual en la que nos encontramos
    ✓ totalPages: Páginas totales que pueden ser solicitadas en la búsqueda.
    ✓ hasNextPage: Indica si es posible avanzar a una página siguiente.
    ✓ nextPage: Página siguiente en la búsqueda
    ✓ hasPrevPage: Indica si es posible retroceder a una página anterior.
    ✓ prevPage: Página anterior en la búsqueda.
    ✓ pagingCounter: Número de documento en relación con la página actual      
    
- Mongoose (properties)
   > index: This will help the database engine to scan through the needed documents and not all of them.
   > virtual: Virtuals are document properties that you can get and set but that do not get persisted to MongoDB. The getters are useful for formatting or combining fields, while setters are useful for de-composing a single value into 
     multiple values for storage.

- Mongoose (middlewares)
   > document: Document middleware is specific to individual documents (instances of a model). It’s triggered during actions like save, validate, remove, updateOne, and deleteOne.
   > model: Model middleware operates on entire collections of documents. It’s triggered during actions like insertMany.
   > query: Aggregate middleware is for operations performed using MyModel.aggregate(). It allows you to modify aggregation pipelines.
   > aggregation: Query middleware lets you modify queries before or after they are executed. It’s triggered during actions like find, findOne, update, and remove.

- Mongoose (functions): Database logic should be encapsulated within the data model. Mongoose provides 2 ways of doing this, methods and statics. Methods adds an instance method to documents whereas Statics adds static "class" methods to the 
  Models itself.
   > methods: It's a method that it's gonna be available on all documents of a certain collection
   > statics: statics are the methods defined on the Model. Those functions are only used ONLY in the model and that exist ONLY in the model.


$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$        

                                                                                                                   Passport
    
- Passport: Generador de estrategias de autenticación y autorización, para mantener un código limpio, estructurado y altamente configurable. Se puede utilizar y configurar múltiples estrategias de autenticación y autorización con passport  
    ✓ Passport local siempre requerirá dos cosas: username y password. Si passport no encuentra alguno de estos dos elementos, devolverá un error y no permitirá proceder con la estrategia
    ✓ Podemos cambiar el campo “username” para que tome el campo que nosotros queramos tomar como identificador, en este caso a nosotros no nos interesa nuestro username, realmente nos interesa el correo electrónico, así que podemos 
      alterarlo con {usernameField: ‘valor’}
    ✓ Passport utiliza un callback “done”, el cual se resuelve de la siguiente manera:
      ○ El primer parámetro de done es el error, si pasamos done(null) indicamos que no hay error.
      ○ El segundo parámetro debe ser el usuario generado, por lo tanto, para devolver un usuario, hacemos done(null, user).
      ○ Si pasamos done(null, false) indicamos que no hay error, pero el usuario no estará disponible.
    ✓ Cada estrategia que queramos configurar en passport es un middleware por sí solo, así que utilizaremos el elemento passport.use() para configurar diferentes middlewares/estrategias

                                                                                                                   
    
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$        

                                                                                                                   Nestjs

- Definicion: Es un framework que permite crear Server Side Applications, de una manera eficiente y escalable. Está desarrollado con una base sólida de Typescript y brinda la posibilidad de seguir utilizando plain javascript. 
  Permite implementar paradigmas combinados de Programación Orientada a Objetos, Programación Funcional, y Programación Funcional reactiva. 
  Es un framework que está basado internamente en Express, y está pensado principalmente para construir aplicaciones monolíticas y microservicios.
  
- Instalación y primer proyecto: npm i -g @nestjs/cli  --> nest new <nombre de proyecto>
- Creacion de un modulo: nest g resource <nombre del modulo>    
        
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$        

                                                                                                                   Arquitectura de capas

- Definicion de aquitectura de capas: Es un patrón de diseño donde los módulos contemplados dentro de nuestro aplicativo son separados por “capas”. El nombre “capa” hace referencia a cada rol que debe cumplirse dentro de todo el aplicativo.                                                                                                                   

  > Responsabilidades: Cuando trabajamos con capas, entendemos que cada archivo debe cumplir una función específica, permitiendo así que, si llegase a ocurrir algún “error” o si llegase a requerirse modificación en algún punto, tengamos más 
    claro dónde debemos atacar esos cambios.
  > Capas base: En un sistema que trabaje con este modelo, es necesario contar con tres capas base:
     ✓ Capa de Modelo o Persistencia: Esta capa tiene por principal objetivo la conexión directa con el modelo de persistencia a trabajar, es decir, debe saber conectar con la persistencia en memoria, en archivos o en bases de datos. Todo 
       dependendiendo de cómo haya sido programada la capa. La capa de persistencia no debería realizar validaciones ni operaciones más allá del CRUD que corresponde a una capa de persistencia. Para modelos más complejos como Persistencias 
       en bases de datos, también es posible configurar operaciones transaccionales y Agregaciones en este mismo punto. 
     ✓ Capa de Vista o Renderización: La capa de renderizado o Vista, como indica su nombre, sólo tiene la función de tomar datos para poder ser renderizados. Esta capa es una de las más subjetivas en la arquitectura, pues si bien TODOS los 
       modelos requieren renderizar contenido, se hace de maneras muy diferentes. Renderizado, según sea el enfoque del equipo, también puede acceder a Persistencia sin necesidad de pasar por negocio, siempre y cuando ésta tenga como fin 
       único el de mostrar la información correspondiente. En ocasiones, también se suele contemplar la capa de renderizado fuera de la arquitectura interna y usar algún aplicativo externo (como enviar la info a React para que él la 
       renderice, que es lo más habitual).
     ✓ Capa de Controlador o Negocio: Esta capa tiene por principal objetivo la conexión directa con el modelo de persistencia a trabajar, es decir, debe saber conectar con la persistencia en memoria, en archivos o en bases de datos. Todo 
       dependendiendo de cómo haya sido programada la capa. La capa de persistencia no debería realizar validaciones ni operaciones más allá del CRUD que corresponde a una capa de persistencia. Para modelos más complejos como Persistencias 
       en bases de datos, también es posible configurar operaciones transaccionales y Agregaciones en este mismo punto. 
    Sin estas tres capas, el modelo se volvería inconsistente y la comunicación entre los módulos sería débil y generaría muchos problemas.
  > Capa de routing:  La capa de routing contendrá todos los archivos de tipo “router” que, como estamos ya acostumbrados, es una capa basada en redireccionamientos hacia puntos específicos de nuestra API. Actualmente, con el uso de motores 
    de plantillas, nuestra capa de routing está estrechamente conectada con la capa derenderización (al utilizar un views router). Sin esta capa, todas nuestras rutas de todas nuestras entidades se encontrarían en un mismo archivo, 
    complicando la lectura del código posteriormente.
  > Capa de servicio: La capa de servicios es una capa intermedia entre el controlador y la persistencia, en esencia, un servicio tiene la capacidad de servir como “tunel” de conexión, para que el controlador pueda acceder de manera más 
    homologada a la persistencia. Contar con una capa de servicio impide que los accesos a persistencia se hagan descontroladamente, con argumentos erróneos, etc. Además, son el punto clave para aplicar un patrón repository. No confundir una 
    capa de servicio con la capa de negocio, solo es un punto intermedio de conexión.

- Analisis del flujo y capas del proyecto: En ella se deben seguir los pasos adecuados para una mejor actitectura. Los cuales son:
  > Inicio: capa de vista, presentación o renderizado: Como es de esperarse, todo comienza desde el cliente, cuando este carga una página, aprieta un botón o desea buscar un dato, está haciendo una petición al servidor. Para ello, desde esta 
    capa se hace un “consumo”, el cual hará una operación de solicitud para obtener los datos.
  > Primer contacto (capa de ruteo): La petición tuvo que realizarse a partir de un endpoint, el cual entra a la capa de ruteo y designa cuál de todas las rutas corresponden a la acción que desea realizar el cliente.
  > Segundo contacto (capa de controlador): Cada ruta está relacionada con algún método o función de un controlador, así, cuando el router reconoce adónde está apuntando el cliente, sabe llevarlo a la función adecuada.
  > Tercer contacto (Capa de Servicio): Para poder obtener, un grupo de usuarios, la función perteneciente a estos usuarios requerirá acceder a un punto intermedio entre el controlador y entre la capa de persistencia para obtener la info.
  > Cuarto contacto (Capa de Persistencia): El punto intermedio mencionado arriba sabe que debe acceder a usuarios, el detalle es ¿de qué persistencia? ¿Memoria, archivos, base de datos? El servicio sabrá a qué persistencia conectar y 
    obtendrá los datos
  > Regreso (vuelta de datos y envío al cliente): Una vez obtenidos los datos a partir de la persistencia, el controlador termina de procesarlos y puede finalmente enviarlos al cliente para terminar el procesamiento de esa petición.
    

                                                                                                                   Patrones de diseño reconocidos en Expressjs
                                                                                                                   
- Cadena de responsabilidades: Permite que, cuando algún elemento envía información (sender) y existe alguien que lo reciba (receiver), a esa petición puedan recibirla y procesarla múltiples objetos (o funciones). Esto permite tener un mejor 
  control de la petición, agregar filtros y reenviar el objeto con sus respectivas alteraciones.   
  Express popularizó enormemente el concepto de un Middleware, que al final es una variante de la inyección de dependencias, donde al recibir un request, éste puede pasar por diferentes lógicas en cada middleware, al final, un middleware 
  responde para recibirse por otro middleware, y así sucesivamente hasta llegar al endpoint principal.
  
- Decorador: Permite mantener un objeto inicial genérico para poder procesar información, pero al ser utilizado éste está abierto a ser transformado a lo largo del flujo del proceso. De no querer que un objeto tenga añadiduras nuevas, 
  podemos congelar el objeto con “Object.freeze()”, sin embargo, rompería con el patrón decorador al no permitir que se le cambie.
  Cuando recibimos un request, el objeto request está predefinido con ciertas propiedades, sin embargo, podemos procesar n middlewares para transformar el request. Al usar multer, por ejemplo, a nuestro endpoint llega una propiedad req.file 
  o req.files, cosa que no teníamos antes del middleware.
  
- Proxy: También conocido como Proxy routing o simplemente Routing pattern, implica tener un sustituto (surrogate), el cual reciba una petición y controlar el acceso hacia otro objeto (subject). El sustituto recibirá todas las peticiones, 
  para después corroborar a quién debería corresponder dicha petición y enviársela. El sustituto y el objeto final deben contar con la misma interfaz. Cuando creamos un nuevo router en la aplicación principal de Express, y conectamos el 
  middleware de router con app.use(), éste se convierte en un sustituto que definirá a cuál router redirigir la información. Es el patrón más común que se puede reconocer en Express, pues son los primeros pasos que damos al comenzar a usarlo

- Patrón MVC: Es un patrón que ya se ha platicado ampliamente en las últimas clases, éste consistiendo en la separación de capas de modelo (persistencia), Vista (presentación) y Controlador (Negocio). Recordemos que al final el objetivo es 
  mantener un flujo con actividades bien delegadas y así poder tener mejor control sobre el código.

- Singleton Pattern: Es un patrón utilizado para tener una instancia global a nivel aplicación. En ocasiones, se requiere que la aplicación tenga una única instancia de dicha clase (Por ejemplo, al abrir una conexión en base de datos). El 
  patrón singleton corrobora si ya existe una instancia de esta clase. En caso de que sí, devolverá la instancia, caso contrario creará la instancia.
  Singleton is a design pattern that ensures that a class has only one immutable instance. Said simply, the singleton pattern consists of an object that can't be copied or modified. It's often useful when we want to have some immutable
  single point of truth for our application.
  
- Null Object Design Pattern: This Pattern wraps up the null into its own object. Instead of having a null reference for some object, we wrap it into a NULL version of that object which will implement the same interface that of the object, 
  i.e. same methods and properties. The intent of a NULL OBJECT is to encapsulate the absence of an object by providing a substitutable alternative that offers suitable default do-nothing-behavior. In short, a design where “nothing will come 
  of nothing”. We can use this pattern whenever we have the NULL object returned or checked against before accessing the properties of the object. It has a kind of do-nothing behavior on its methods.

- Facade Design Pattern: Facade design pattern is a Structural design pattern that allows users to create a simple interface that hides the complex implementation details of the system making it easier to use. This pattern intends to create 
  a simplified and unified interface for a set of interfaces hiding the implementation details making it less complex to use. The components of the Facade design pattern include:
    > Subsystem: The Subsystem is a class or group of classes or interfaces that handles the entire complex logic and implementation.
    > Facade: The Facade is the class or object that serves as an entry point to the Subsystem’s implementation hiding the complex implementation details.
    > Client: The Client interacts with the simplified Interface provided by Facade to perform operations without knowing the internal implementation details.

- Builder Pattern: This pattern is used to create objects in "steps". Normally we will have functions or methods that add certain properties or methods to our object. The cool thing about this pattern is that we separate the creation of 
  properties and methods into different entities. If we had a class or a factory function, the object we instantiate will always have all the properties and methods declared in that class/factory. But using the builder pattern, we can create 
  an object and apply to it only the "steps" we need, which is a more flexible approach.

- Command Pattern: With the Command Pattern, we can decouple objects that execute a certain task from the object that calls the method. The Command Pattern is a behavioral design pattern that converts requests or simple operations into 
  objects. The key idea here is that it encapsulates a request as an object, thereby letting us parameterize clients with queues, requests, operations, and also allows us to support undoable operations.
  The essence of this pattern involves three main components:
   > The Command: This is an interface that specifies how to carry out a given operation.
   > The Invoker: This is a component that uses the command to carry out a given operation.
   > The Receiver: This is the component that knows how to perform the operations associated with the command.
   
- Prototype Pattern: The Prototype pattern allows you to create an object using another object as a blueprint, inheriting its properties and methods. It is a useful way to share properties among many objects of the same type. The prototype 
  is an object that’s native to JavaScript, and can be accessed by objects through the prototype chain. In our applications, we often have to create many objects of the same type. A useful way of doing this is by creating multiple instances 
  of an ES6 class.
  
- Service: Es un archivo que va a contener todos los metodos que podemos llamar desde cualquier parte de la aplicacion.

- Factory Method Pattern: The Factory method pattern provides an interface for creating objects that can be modified after creation. The cool thing about this is that the logic for creating our objects is centralized in a single place, 
  simplifying and better organizing our code. This pattern is used a lot and can also be implemented in two different ways, via classes or factory functions (functions that return an object).
  
- Abstract Factory Pattern: This pattern allows us to produce families of related objects without specifying concrete classes. It's useful in situations where we need to create objects that share only some properties and methods.
  The way it works is by presenting an abstract factory the client interacts with. That abstract factory calls the corresponding concrete factory given the corresponding logic. And that concrete factory is the one that returns the end 
  object. Basically it just adds an abstraction layer over the factory method pattern, so that we can create many different types of objects, but still interact with a single factory function or class.
  
  
                                                                                                                   Arquitectura del servidor: Persistencia
                                                                                                                   
- Memoria: La información persiste sólo durante el ciclo de vida del programa, si se reinicia, la información desaparece.
- Archivos: La información se guarda en un fichero que permite persistir la información, aun cuando el programa se reinicie o apague.
- Base de datos: Permite que no solo se pueda almacenar información, sino también facilita la aplicación de un CRUD de manera rápida y segura. Es la persistencia por excelencia.

- DAO: Se encargará de conectar con nuestra fuente de datos según la hayamos programado, habrá DAOs programados para hacer CRUD en memoria, DAOs para hacer CRUD en archivos, etc. Así, en la lógica de negocio sólo se necesita importar el DAO 
  a trabajar y utilizarlo. Si en algún momento necesitamos cambiar de persistencia, bastará con cambiar el DAO

- El patrón DAO (persistencia aislada): consiste en separar la lógica de acceso a la fuente de datos en un archivo. Éste contará con métodos homologados de manera que, si en algún momento necesitamos cambiar el acceso a los datos, el DAO de 
  la nueva fuente de datos tenga exactamente el mismo nombre de métodos que el anterior, evitando así que haya conflictos al acceder a la información. Así, podemos tener un MemoryDAO, un fileDAO, un databaseDAO según sea el caso, e
  intercambiarlos sin problema
  
- Patrón Factory: La idea del patrón Factory, es basarse en una variable de entorno o configuración por argumentos, la cual tomará para decidir qué tipo de persistencia manejar. Esta “Fábrica” se encargará de devolver sólo el DAO que 
  necesitemos acorde con lo solicitado en el entorno o los argumentos. 
  
- Patron repository: 

- Patron service: 

- Data Transfer Object: DTOs or Data Transfer Objects are objects that carry data between processes in order to reduce the number of methods calls. The pattern’s main purpose is to reduce roundtrips to the server by batching up multiple 
  parameters in a single call. This reduces the network overhead in such remote operations. Another benefit is the encapsulation of the serialization’s logic (the mechanism that translates the object structure and data to a specific format 
  that can be stored and transferred). It provides a single point of change in the serialization nuances. It also decouples the domain models from the presentation layer, allowing both to change independently.


$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$        


                                                                                                                   Data Modeling
                                                                                                                   
- Data Modeling: Data modeling refers to the organization of data within a database and the links between related entities. Data in MongoDB has a flexible schema model, which means: 
    + Documents within a single collection are not required to have the same set of fields. 
    + A field's data type can differ between documents within a collection. 
  Generally, documents in a collection share a similar structure. To ensure consistency in your data model, you can create schema validation rules.
  
  > Use Cases: The flexible data model lets you organize your data to match your application's needs. MD is a document database, meaning you can embed related data in object and array fields. A flexible schema is useful according to:
     + Your company tracks which department each employee works in. You can embed department information inside of the employee collection to return relevant information in a single query.
     + Your e-commerce application shows the five most recent reviews when displaying a product. You can store the recent reviews in the same collection as the product data, and store older reviews in a separate collection because the older 
       reviews are not accessed as frequently.
     + Your clothing store needs to create a single-page application for a product catalog. Different products have different attributes, and therefore use different document fields. You can store all of the products in the same collection.
  
  > Link Related Data: When you design your data model in MongoDB, consider the structure of your documents and the ways your application uses data from related entities. To link related data, you can either: 
    + Embed related data within a single document.
    + Store related data in a separate collection and access it with a  reference.

- Denormalization: Denormalization is used to combine multiple table data into one so that it can be queried quickly. It is a process of storing the join of higher normal form relations in the form of base relation that is in a lower normal 
  form. The primary goal of denormalization is to achieve the faster execution of the queries.
  In the process of denormalization, the data is integrated into the same database. Denormalization is mainly used where joins are expensive and queries are executed on the table very frequently. However, there is a drawback of 
  denormalization, that is, a small wastage of memory.

- Normalization: Normalization is used to remove redundant data from the database and to store non-redundant and consistent data into it. It is a process of converting an unnormalized table into a normalized table. Database normalization is 
  an important process because a poorly designed database table is inconsistent and may create issues while performing operations like insertion, deletion, updating, etc. 
  The process of Normalization involves resolution of database anomalies, elimination of data redundancy, data dependency, isolation of data, and data consistency. Normalization in databases provides a formal framework to analyze the 
  relations based on the key attributes and their functional dependencies. It reduces the requirements of restructuring of tables.
    > Child referencing: The Child References pattern stores each tree node in a document; in addition to the tree node, document stores in an array the id(s) of the node's children. Each node contains a reference array to its children
    > Parent referencing: The Parent References pattern stores each tree node in a document; in addition to the tree node, the document stores the ID of the node's parent. Each node contains a reference to its parent.
    
- Difference between Normalization and Denormalization: The following table highlights the important differences between Normalization and Denormalization
  > Normalization
    + Implementation: Normalization is used to remove redundant data from the database and to store non-redundant and consistent data into it.	
    + Focus: Normalization mainly focuses on clearing the database from unused data and to reduce the data redundancy and inconsistency.	
    + Number of Tables: During Normalization, data is reduced, so there will be a decrease in the number of tables.	
    + Memory consumption: Normalization uses optimized memory and hence faster in performance.	
    + Data integrity: Normalization maintains data integrity, i.e., any addition or deletion of data from the table will not create any mismatch in the relationship of the tables.	
    + Where to use: Normalization is generally used where a number of insert/update/delete operations are performed and joins of those tables are not expensive.	
  
  > Denormalization
    + Implementation: Denormalization is the process of adding some redundant data to a database that has been normalized, so as to improve the read performance (execution time) of the database
    + Focus: The real goal of denormalization is to achieve the faster execution of the queries by introducing redundancy.
    + Number of Tables: During Denormalization, data is integrated into the same database and hence there will be an increase in the number of tables.
    + Memory consumption: Denormalization introduces some sort of wastage of memory.
    + Data integrity: Denormalization does not maintain any data integrity.
    + Where to use: Denormalization is used where joins are expensive and frequent queries are executed on the tables.

- Embedded Documents: Embedded documents store related data in a single document structure. A document can contain arrays and sub-documents with related data. These denormalized data models allow applications to retrieve related data in a 
  single database operation. For many use cases in MongoDB, the denormalized data model is optimal.
  Embedded documents are stored as children inside a parent document. This means they are all stored under one collection, and whenever you retrieve the parent document, you also retrieve all its embedded documents
  For example, if you had a user document, it may contain a list of embedded documents detailing that user’s addresses. In JSON format, it may look something like this:
    
        {
          "_id": 1,
          "name": "Ashley Peacock",
          "addresses": [
            {
              "address_line_1": "10 Downing Street",
              "address_line_2": "Westminster",
              "city": "London",
              "postal_code": "SW1A 2AA"
            },
            {
              "address_line_1": "221B Baker Street",
              "address_line_2": "Marylebone",
              "city": "London",
              "postal_code": "NW1 6XE"
            }
          ]
        }
     By storing our addresses as embedded documents, we’re only persisting a single document.
        
- References: References store relationships between data by including links, called references, from one document to another. For example, a customerId field in an orders collection indicates a reference to a document in a customers 
  collection. Applications can resolve these references to access the related data. Broadly, these are normalized data models.
  Unlike embedded documents, referenced documents are stored in a separate collection to their parent document. Therefore, it’s possible to retrieve the parent document without retrieving any of its referenced documents.

        // Stored in the user collection
        {
          "_id": 1,
          "name": "Ashley Peacock",
          "addresses": [
            1000,
            1001
          ]
        }
        // Stored in the address collection
        {
          "_id": 1000,
          "address_line_1": "10 Downing Street",
          "address_line_2": "Westminster",
          "city": "London",
          "postal_code": "SW1A 2AA"
        }
        // Stored in the address collection
        {
          "_id": 1001,
          "address_line_1": "221B Baker Street",
          "address_line_2": "Marylebone",
          "city": "London",
          "postal_code": "NW1 6XE"
        }
        
  In storing our addresses using references, we’re storing 3 separate documents. If we want to retrieve a user’s addresses along with the user, we must effectively run 2 queries: the first to retrieve the user, the second to retrieve 
  their addresses. This is because MongoDB, unlike SQL, has no performant concept of a join. It does provide a lookup operation, but it’s not typically performant enough to use in a real time production environment — both in terms of 
  speed and resources. Instead, we should look to better design our schemas to suit MongoDB, which is what we’ll cover next. 
  It’s worth noting we can choose how we model the relationship when using references. We can decide, as we have above, to store a list of references on the parent document. If the list of references is likely to become large, it’s 
  usually better to store the ID of the parent document (user in this case) in any related documents (in the address in this case). Otherwise, each time we add a new address, we have to update the user document to add the new reference.
     
- How To Choose Between Embedded Or Referenced Documents: Generally speaking, we should look to use embedded documents when both the parent document and its related documents are either read or written at the same time. Furthermore, we 
  should prioritise based on whether the collection is read or write heavy. For example, consider the following document:
        {
          "_id": 1,
          "name": "Ashley Peacock",
          "account_balance": 150
          "stocks": [
            {
              "code": "AMZN",
              "amount": "3",
              "value": "1478.22"
            }
          ],
          "orders": [
            {
              "order_id": 1,
              "order_value": 102.5,
              "order_items": [...]
            }
          ]
        }
        
  We have two embedded documents: stocks and orders. Should they be embedded, though? To answer that question, we need a little more context — here’s what the fictional stock trading application looks like in terms of how we interact with 
  our user collection: 
     + Once logged in, the user ID is stored in the web session.
     + Stocks contain the stocks the user currently owns. They are shown on almost every page of the website, and must be retrieved from the database each time a page is rendered for compliance and accuracy reasons. Stocks are updated 
       infrequently when compared to how often they are read.
     + The account balance is also shown on every page.
     + Orders are only shown on the orders page, and only updated when an order is placed.
     
  In this case, I would store the stocks as embedded documents because whenever we render a page we need both the account balance and the list of stocks. If we stored them separately, we’d have to execute two separate queries — one to get 
  the user, another to get their stocks. Considering this data is shown on every page, we’d be doubling the number of queries per page load if we referenced them.
  Moving on to the orders, I would store these as referenced documents, as they are only used on specific pages. Furthermore, we can query for orders, and add new orders, simply by using the user ID stored in the web session — without 
  querying the user collection at all.
  A user could have 100’s of orders. If we stored them as embedded documents, the amount of data returned for the user query would grow significantly over time. Additionally, it would return a large amount of data that isn’t used on the 
  majority of the website. To add a new order, we would need to read the entire user document, append the new order, and then re-write the entire document.
  Using referenced documents however, we can simply insert a new document into the orders collection, tied to the user ID, and we don’t have to read the user collection at all (or any other orders!).
