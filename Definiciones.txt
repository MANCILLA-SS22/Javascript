                                                                                                       Conceptos y definiciones
                                                                                                                   
- Callback function: Aseguran que una función no se va a ejecutar antes de que se complete una tarea, sino que se ejecutará justo después de que la tarea se haya completado. Nos ayuda a desarrollar código JavaScript asíncrono y nos 
  mantiene a salvo de problemas y errores. Una función callback es aquella que es pasada como argumento a otra función para que sea "llamada de nuevo" (call back) en un momento posterior. Una función que acepta otras funciones como 
  argumentos es llamada función de orden-superior (High-Order), y contiene la lógica para determinar cuándo se ejecuta la función callback. Es la combinación de estas dos la que nos permite ampliar nuestra funcionalidad. La forma de crear 
  una función callback es pasándola como parámetro a otra función, y luego llamarla de vuelta justo después de que haya ocurrido algo o se haya completado alguna tarea.
  
  A callback function is a function passed into another function as an argument, which is then invoked inside the outer function to complete some kind of routine or action.
  Callbacks make sure that a function is not going to run before a task is completed but will run right after the task has completed. It helps us develop asynchronous JavaScript code and keeps us safe from problems and errors.
  In JavaScript, the way to create a callback function is to pass it as a parameter to another function, and then to call it back right after something has happened or some task is completed. 

- fetch: Fetch is the retrieval of data by a software program, script, or hardware device. After being retrieved, the data is moved to an alternate location or displayed on a screen.

- Hoisting: Es el proceso de mover todas las declaraciones de variables a la parte superior de la función o del entorno. El concepto de hoisting de JavaScript determina que el valor de una variable declarada con var puede subir al 
  principio del scope de la función dentro de la que está declarada. Esto puede ser peligroso, porque puedes terminar con un valor de tipo undefined a pesar de haberle otorgado valor a tus variables. Es decir, la palabra clave var a veces 
  nos sorprende con valores indefinidos en variables definidas. El hoisting de JavaScript raras veces genera este comportamiento en variables declaradas con const y con let. Aunque puede suceder, el desarrollador web casi que tiene que 
  generar este comportamiento a propósito para que suceda.                                                                                                                
  
-  Node JS: Node.js es un entorno de ejecución de JavaScript, que le permite al código en js ser ejecutado en nuestra computadora. Podemos darle a node un archivo de js y éste puede ejecutarlo, y darle acceso a recursos de nuestra 
   computadora (IO, Network, Etc). El “Node” de java es conocido como la JVM.
   
-  NPM (Node Packager Manager): Cuando usamos Node.js, rápidamente tenemos que instalar módulos nuevos (librerías), ya que al ser un sistema fuertemente modular viene prácticamente “vacío”. Por lo tanto, para utilizar una funcionalidad de 
   alguna librería publicada, deberemos instalar módulos adicionales. Esta operación se realiza de forma muy sencilla con la herramienta npm (Node Package Manager).
   Esta herramienta funciona de dos formas:
     ✓ Como un repositorio ampliamente utilizado para la publicación de proyectos Node.js de código abierto.
     ✓ Como una herramienta de línea de comandos. Esta utilidad ayuda a instalar y desinstalar paquetes, gestionar versiones y gestionar dependencias necesarias para ejecutar un proyecto.

- JSON (JavaScript Object Notation): It is a format for storing and transporting data. JSON is text, and text can be transported anywhere, and read by any programming language. Es un formato basado en texto plano de intercambio de datos 
  totalmente independiente del lenguaje de programación, para representar datos estructurados con la sintaxis de objetos de JavaScript. Es comúnmente utilizado para enviar y almacenar datos o información estructurada en aplicaciones web, 
  entre un servidor y un cliente. Aunque es muy parecido a la sintaxis de JavaScript, puede ser utilizado independientemente de JavaScript, y muchos entornos de programación poseen la capacidad de leer (convertir; parsear) y generar JSON. 
  JSON es un string con un formato específico. Otros puntos a tener en cuenta son:
  
     ~ Los datos contenidos en un archivo en formato JSON deben estructurarse por medio de una colección de pares con nombre y valor o deben ser una lista ordenada de valores. Sus elementos tienen que contener:
        + Clave: corresponde al identificador del contenido. Por eso, debe ser una string delimitada por comillas.
        + Valor: representa el contenido correspondiente y puede contener los siguientes tipos de datos: string, array, object, number, boolean o null.
     ~ Las datos en formato JSON se pueden almacenar en archivos externos .json. Exemplo: datos.json 
     ~ JSON es sólo un formato de datos - contiene sólo propiedades, no métodos.
     ~ A diferencia del código JavaScript en que las propiedades del objeto pueden no estar entre comillas, en JSON sólo las cadenas entre comillas pueden ser utilizadas como propiedades.
     ~ JSON es solo un formato de datos.
     ~ Requiere usar comillas dobles para las cadenas y los nombres de propiedades. Las comillas simples no son válidas. 
     ~ Puede tomar la forma de cualquier tipo de datos que sea válido para ser incluido en un JSON, no solo arreglos u objetos. Así, por ejemplo, una cadena o un número único podrían ser objetos JSON válidos.
     ~ A diferencia del código JavaScript, en el que las propiedades del objeto pueden no estar entre comillas, en JSON solo las cadenas entre comillas pueden ser utilizadas como propiedades.
      
  La transferencia de datos entre aplicaciones es realizada por medio de API —Application Programming Interface— que, entre otros formatos, utiliza la notación JSON para estructurar la información enviada.
  El archivo JSON también se usa para realizar requisiciones AJAX en sitios web, en que se hacen diferentes interacciones con bancos de datos, como el MySql, para realizar operaciones como consulta, inclusión y exclusión de registros.
  
  Principales casos de uso para JSON:
     ~ Generación de un objeto JSON a partir de datos generados por el usuario: JSON es perfecto para el almacenamiento de datos temporales. Por ejemplo, los datos temporales pueden ser generados por el usuario, como un formulario enviado en 
       un sitio web. JSON también se puede utilizar como formato de datos para cualquier lenguaje de programación a fin de incrementar el nivel de interoperabilidad.
     ~ Tansferencia de datos entre sistemas: Una base de datos de sitio web tiene la dirección postal de un cliente, pero la dirección debe verificarse a través de una API para garantizar su validez. Envío de los datos de dirección en 
       formato JSON a la API de servicio de validación de direcciones.
     ~ Configuración de datos para aplicaciones: Al desarrollar aplicaciones, cada aplicación necesita las credenciales para conectarse a una base de datos, así como una ruta de acceso al archivo log. Las credenciales y la ruta de acceso al 
       archivo se pueden especificar en un archivo JSON que sea legible y que esté siempre disponible.
     ~ Simplificación de modelos de datos complejos: JSON simplifica los documentos complejos hasta los componentes que se han identificado como significativos mediante la conversión del proceso de extracción de datos en un archivo JSON 
       predecible y legible por humanos.
     
- API (Application Programming Interfaces): que en español significa interfaz de programación de aplicaciones. Es una aplicación web construida en base a la arquitectura API REST, a la cual podemos solicitar y enviar información desde el 
  cliente. Tambien, se define como un conjunto de definiciones y reglas que permiten que dos equipos puedan integrarse para trabajar juntos. La mejor analogía que hay para comprender ésto es que una API funge como un “contrato” entre el 
  front y el back. La API permite entonces que se respondan preguntas como:
    ✓ ¿A qué endpoint debo apuntar para la tarea que necesito?
    ✓ ¿Qué método debo utilizar para ese recurso?
    ✓ ¿Qué información debo enviar para realizar correctamente mi petición? 
    
    https://www.ibm.com/topics/api
    https://stoplight.io/api-types#web-apis
    
  Generalmente, nos comunicamos con aplicaciones de este tipo y es la tendencia actual de desarrollo. La ventaja de este modelo es que está orientado a recursos y define métodos claros para solicitar y enviar información. Una API
  suele tener una URL base (el dominio donde está alojada la aplicación) y luego puede tener varios endpoints, es decir, distintas secciones a las que podemos acceder. A la vez, se pueden hacer peticiones con distintos métodos al mismo 
  endpoint y obtener distintos resultados. Generalmente, similar a cuando queremos incorporar una librería, al momento de consumir una API debemos revisar su documentación. Allí se definen los distintos endpoints disponibles, los métodos a 
  utilizar para hacer una petición y qué se nos ofrecerá en respuesta.
  Se trata de un conjunto de definiciones y protocolos que se utiliza para desarrollar e integrar el software de las aplicaciones, permitiendo la comunicación entre dos aplicaciones de software a través de un conjunto de reglas. Así pues, 
  podemos hablar de una API como una especificación formal que establece cómo un módulo de un software se comunica o interactúa con otro para cumplir una o muchas funciones. Todo dependiendo de las aplicaciones que las vayan a utilizar, y de 
  los permisos que les dé el propietario de la API a los desarrolladores de terceros.
  
  Una de las principales funciones de las API es poder facilitarle el trabajo a los desarrolladores y ahorrarles tiempo y dinero. Por ejemplo, si estás creando una aplicación que es una tienda online, no necesitarás crear desde cero un 
  sistema de pagos u otro para verificar si hay stock disponible de un producto. Podrás utilizar la API de un servicio de pago ya existente, por ejemplo PayPal, y pedirle a tu distribuidor una API que te permita saber el stock que ellos 
  tienen. Con ello, no será necesario tener que reinventar la rueda con cada servicio que se crea, ya que podrás utilizar piezas o funciones que otros ya han creado. Imagínate que cada tienda online tuviera que tener su propio sistema de 
  pago, para los usuarios normales es mucho más cómodo poder hacerlo con los principales servicios que casi todos utilizan. También son útiles para cuando lo único que se quiere es utilizar deliberadamente las funciones de determinado 
  servicio para ofrecer ventajas a sus usuarios o atraer a los usuarios de ese servicio a que utilicen tu aplicación.
  
- API protocols: Understanding what protocol an API uses is just as important as knowing what type it is. The protocol defines how your API connects to the internet and how it communicates information. The protocol you choose will determine 
  how you design and build your API, as well as what’s required to maintain it, so it’s important to understand the advantages and drawbacks of each choice.  

 > Api rest: Una API REST es una interfaz de comunicación entre sistemas de información que usa el protocolo de transferencia de hipertexto (hypertext transfer protocol o HTTP, por su siglas en inglés) para obtener datos o ejecutar 
   operaciones sobre dichos datos en diversos formatos, como pueden ser XML o JSON.     
     1) REST: Ya tenemos las reglas para comunicarse, ¿Pero qué tal la estructura del mensaje? Cuando hacemos una petición o cuando recibimos una respuesta, ésta debe tener un formato. REST (REpresentational State Transfer) permite definir 
        la estructura que deben tener los datos para poder transferirse. La API respondía a preguntas sobre cómo comunicarse correctamente, sin embargo, REST define cómo debe ser el cuerpo del mensaje a transmitir. (puedes llegar a hablar 
        con el  presidente si cumples con el protocolo (HTTP) y las reglas (API), pero ¿de qué nos servirá si la forma en que estructuramos nuestro mensaje (REST) no es correcta?).
       Los dos formatos más importan tes son JSON y XML. La utilización de la estructura dependerá de las necesidades del proyecto. Nosotros utilizaremos JSON. Como otarás, ¡un JSON parece un objeto! así que es mucho más amigable la sintaxis
     2) API REST es: Un modelo completo para tener perfectamente estipulados los protocolos, las reglas, e incluso la estructura de la información, con el fin de poder hacer un sistema de comunicación completo entre las computadoras.
     3) Características debe tener una API REST: Estas son las caracteristicas mas importantes de una API REST
        a) Arquitectura Cliente-Servidor sin estado: 
           ~ Cada mensaje HTTP contiene toda la información necesaria para comprender la petición. 
           ~ Como resultado, ni el cliente ni el servidor necesitan recordar ningún estado de las comunicaciones entre mensajes.
           ~ Esta restricción mantiene al cliente y al servidor débilmente acoplados: el cliente no necesita conocer los detalles de implementación del servidor y el servidor se “despreocupa” de cómo son usados los datos que envía al cliente
        b) Cacheable: 
           ~ Debe admitir un sistema de almacenamiento en caché.  
           ~ La infraestructura de red debe soportar una caché de varios niveles. 
           ~ Este almacenamiento evita repetir varias conexiones entre el servidor y el cliente, en casos en que peticiones idénticas fueran a generar la misma respuesta.
        c) Operaciones comunes:
           ~ Todos los recursos detrás de nuestra API deben poder ser consumidos mediante peticiones HTTP, preferentemente sus principales (POST, GET, PUT y DELETE).
           ~ Con frecuencia estas operaciones se equiparan a las operaciones CRUD en bases de datos (en inglés: Create, Read, Update, Delete, en español: Alta, Lectura, Modificación, y Baja).  
           ~ Al tratarse de peticiones HTTP, éstas deberán devolver con sus respuestas los correspondientes códigos de estado, informando el resultado de las mismas. 
        d) Interfaz uniforme: 
           ~ En un sistema REST, cada acción (más correctamente, cada recurso) debe contar con una URI (Uniform Resource Identifier), un identificador único.  
           ~ Ésta nos facilita el acceso a la información, tanto para consultarla, como para modificarla o eliminarla, pero también para compartir su ubicación exacta a terceros.
        e) Utilización de hipermedios: 
           ~ Cada vez que se hace una petición al servidor y este devuelve una respuesta, parte de la información devuelta pueden ser también hipervínculos de navegación asociada a otros recursos del cliente.
           ~ Como resultado de esto, es posible navegar de un recurso REST a muchos otros, simplemente siguiendo enlaces sin requerir el uso de registros u otra infraestructura adicional
   
 > SOAP API: The Simple Object Access Protocol (SOAP) is another major API protocol. A SOAP API can communicate over other major internet communication protocols, such as TCP and SMTP, in addition to HTTP. In that regard, it is more flexible 
   than REST, but in most ways, SOAP is more restrictive. SOAP APIs can only work with XML data and have much more rigid requirements for requests. SOAP requests also generally require more bandwidth than REST, and building and maintaining 
   SOAP code is more complex.
   One major advantage of SOAP is that it requires metadata files describing requests, which makes exchanges more predictable. It also enables stateful requests, unlike REST, which is stateless. Having a more standardized protocol allows    
   SOAP APIs to communicate more complex data reliably, and to deliver it over more channels than just HTTP. SOAP’s use of service interfaces instead of simple URL-based organization can also lead to greater discoverability for knowledgeable 
   users. In general, SOAP is a better fit for more sophisticated applications, where reliability is more important than speed or usability by a public audience. As a result, it’s widely used in financial services and in large enterprise 
   applications like Salesforce.

 > RPC API: The Remote Procedure Call (RPC) protocol can return XML or JSON responses. It differs from SOAP and REST APIs in a few key ways. As the name suggests, this protocol calls a method rather than a data resource. While a RESTful API 
   returns a document, the response from an RPC server is confirmation that the function was triggered, or an error indicating why it failed to run. In other words, a REST API works with resources, while an RPC API works with actions.
   Another key difference is that a REST API shows the server and the query parameters in its routes, while an RPC’s URI identifies only the server. RPC APIs are rarely public APIs; triggering methods on remote servers is not something most 
   companies want to allow for the general public. Calling an RPC server actually changes the state of the server, so it goes beyond the stateless/stateful distinction between REST and SOAP. As a result, RPC APIs must have a high level of 
   security and trust between producers and consumers, which is why they are most often private APIs. Discoverability and predictability are thus less important for RPC APIs than they are for REST or SOAP APIs, while reliability and 
   performance are more important.
   One of the most common use cases for RPC APIs is distributed client-server applications. Payloads are light and limited to parameters for the methods being called, and front-end developers can access server methods without worrying about 
   details like opening and closing connections or parsing inputs. Methods can be called from remote locations, meaning client applications can be hosted entirely separately from the remote backend server that hosts the functions and data. 
   Task threading is also simplified, compared to calling methods locally, because a multi-threaded process can run on the remote server without impacting the client application.   
   
 > GraphQL API: While GraphQL isn’t really a separate protocol, it is a distinct query language, with best practices for its use. GraphQL uses HTTP, similar to a REST API, transmitting text data in the payload of each request, but its 
   approach is different. A REST API has multiple endpoints, each representing a different data schema. To get the information you need, you must map your requirements to the existing schema and call the appropriate endpoints. GraphQL APIs 
   typically have a single endpoint, but effectively unlimited data schemas available at that endpoint. The API user must know what data fields are available, but they can write a query that combines those fields in whatever order they want. 
   Queries are sent in the payload of an HTTP POST request, and data is returned in the shape of the schema specified by the query.
   GraphQL provides users a lot of flexibility within a single query, compared to a REST API’s strict routing requirements. It can also make caching data a challenge and makes the API consumer responsible for maintaining consistent query 
   syntax to get comparable data. Additionally, to use a GraphQL API, the user must know what fields exist in order to write a query. For users to get the most out of a GraphQL API, you will have to provide more extensive custom 
   documentation than for a comparable REST API, and there are fewer tools available to automate the process. If speed is a priority, either for deployment or integration, it may make sense to stick with a more formulaic protocol like REST.   
   
- Peticiones HTTP: El mecanismo por el cual se piden y proveen datos a través de internet es HTTP (Hypertext Transfer Protocol). Cuando emitimos una orden al navegador, hace una petición (o request) HTTP a algún servidor. Luego, la recibirá, 
  procesará y nos devolverá una respuesta con información que utilizaremos en la aplicación. Estas peticiones que debemos hacer están definidas por varias partes
  
  > Una URL o dirección: Cuando nos comunicamos con un servidor para pedir información lo hacemos a través de una URL, ya que éste es un programa alojado en algún host y nos comunicamos con él a través de la dirección correcta
  
  > MÉTODO (method): Un método es una definición que forma parte del protocolo HTTP, el cual nos sirve para canalizar el tipo de petición queestoy realizando sobre un cierto endpoint. De esta manera, el cliente puede llamar al mismo 
    endpoint, pero con diferentes métodos, indicando qué operación quiere realizar con dicho recurso.
    Cada petición que hacemos está acompañada por un verbo que indica al servidor cuál es nuestra intención. El servidor tiene la capacidad de escuchar distintas peticiones en la misma URL, decidir a cuál responder y cómo.     
    Son 4 los verbos más utilizados, aunque hay muchos más: Get, Post, Put & Delete. Las peticiones de tipo POST y PUT van acompañadas de un body (cuerpo de la request) donde se definen los datos o información a enviar al servidor. GET o 
    DELETE, por su parte, no tienen body ya que no necesitan enviar datos adjuntos.
   
    ~ GET: Para obtener información (o recurso) del servidor. Suelen ser las más utilizadas.
   
    ~ POST: Para enviar información al servidor para crear algún recurso.  Significa que el método de la petición será POST Si no lo modificamos será de tipo GET por defecto. Sirve para “crear” recursos, POST se utiliza para operaciones 
      donde no necesitamos obtener un recurso, sino añadir uno. Algunos de los casos donde se utilizan son: Registrar un usuario: Loguear un usuario, Crear un producto, Crear carrito de compra, Enviar información para un correoelectrónico. 
      Se apoya del recurso req.body, donde el body representa la información que el cliente envía para crear. 
    
    ~ PUT: Para crear o modificar algún recurso en el servidor. En este caso se deben mandar todos los elementos contenidos en el objeto aunque solo se desee modificar unos solo. 
      
    ~ PATCH: quiero alterar parcialmente. Aqui, unicamente se puede enviar un elemento de todos los contenidos en el objeto. 
    
    ~ DELETE: Para eliminar algún recurso en el servidor
    
    Estos verbos Nos permiten definir una manera de explicarle al servidor la dirección y nuestras intenciones. Ningún verbo representa una seguridad y/u obligación. Pero si el servidor y el consumidor los respetan, se pueden lograr algunas 
    mejoras como por ejemplo: El navegador sabe que un POST no debería ser cacheado, si hacemos un GET y fuera cacheable el navegador podrá cachearlo, pero nunca lo hará con un recurso con verbo POST
    
    ~ BODY: Es el espacio en la petición donde se definen los datos a enviar al servidor. Aquí se adjuntan los datos a enviar al servidor. En este caso se envía un objeto con la forma { title, body, userId }. 
      El body debe enviarse en formato JSON, por eso lo vemos envuelto en un JSON.stringify(). Se utiliza para transferir piezas de información entre el cliente y el servidor.
      
    ~ HEADERS: Las cabeceras (headers) HTTP permiten al cliente y servidor enviar información sobre la petición y la respuesta. Los headers incluyen información sobre la petición para establecer una transferencia segura y clara, y de ser 
      necesario se pueden modificar para agregar datos adicionales. No debemos confundir información sobre la petición (headers) con los datos que la petición puede transferir (body).
      En este caso se agrega una propiedad ‘Content-type’, con el valor que nos indica la documentación de la API Si no se agrega la petición sería rechazada por el servidor.
      Headers: Se usan para: Definir las respuestas soportadas, requeridas o preferidas. Agregar información extra. Auth tokens, cookies. Lenguaje preferido. Si acepta contenido cacheado. Lo que quieras en forma de texto.
      
    ~ Parámetros (Query Params o URL Params): Para especificar una petición, se puede enviar información adicional en la forma de parámetros a través de la URL. Tenemos dos formas de definir parámetros a través de la URL: 
      ● Query params: Permite adjuntar en la URL una serie de parámetros en la forma de pares clave-valor. Por ejemplo, si queremos buscar algo por google, debemos enviarle un valor de búsqueda por el parámetro q, a través de la url. Se 
        utiliza el símbolo ? para indicar el final de la parte de la dirección de la url y el comienzo del query. A partir de ahi, se escriben parámetros con la Forma clave=valor, pudiendo definir varios separándolos con el signo (&).
      ● URL params: Esta sintaxis permite enviar parámetros directamente en la forma de segmentos de la URL, es decir separados por / . Por ejemplo, la PokeApi nos indica lo siguiente:     https://pokeapi.co/api/v2/pokemon/{id or name}/ 
        Significa que ese {id or name} es un parámetro, un valor dinámico que insertamos en la URL, en este caso para obtener información sobre un pokemon según su ID o nombre. Para obtener aquel con id = 1, haríamos una petición GET a la
        siguiente url:       https://pokeapi.co/api/v2/pokemon/1    
        
      Nos permiten incluir en la dirección información que se usa para especificarle al receptor parámetros para efectuar una búsqueda, son más comunes para buscar recursos que no tengo la seguridad de 
      que existan. Se puede leer como:
       + busca en google.com.ar
       + utilizando https…
       + el recurso search (resultados de búsqueda) …
       + que contengan la palabra (q = query) ‘coderhouse’
       + Se separa la URL de los parámetros utilizando un signo de pregunta ?
       + Cada parámetro tendra key=value & key2=value2
       + Cada parámetro se puede separar por &
       + http://url.com/find?type=order&id=1234
       
  > Principios
    ✓ Una aplicación RESTful requiere un enfoque de diseño distinto a la forma típica de pensar en un sistema: lo contrario a RPC
    ✓ RPC (Remote Procedure Calls, llamadas a procedimientos remotos) basa su funcionamiento en las operaciones que puede realizar el sistema (acciones, usualmente verbos). Ej: getUsuario()
    ✓ En REST, por el contrario, el énfasis se pone en los recursos (usualmente sustantivos), especialmente en los nombres que se le asigna a cada tipo de recurso. Ej. Usuarios.
    ✓ Cada funcionalidad relacionada con este recurso tendría sus propios identificadores y peticiones en HTTP.
       
     ~ URL Params: Son una convención para incluir el identificador del recurso dentro de la misma url, son más comunes cuando ya se conoce el recurso específico que se buscará.
     
     ~ RECURSOS/RESTFUL: Cuando se crea y provee un servicio basado y pensado en términos de recursos, y se respetan las convenciones de verbo/método y código de respuesta, estamos frente a un diseño arquitectural de tipo REST.
       Si además transferimos javascript o xml, es conocido como AJAX

- Query: Es una solicitud pero con ciertos parametros de busqueda. Se entiende por query o consulta cualquier petición de datos hacia un servicio que los devuelva, como puede ser una base de datos. Dentro de las bases de datos, Query hace 
  referencia a la repetición de datos que se encuentran almacenados. Por otra parte, Query también es el término de consulta que deriva en una SERP dentro de los buscadores. De esta manera, esta herramienta tiene como objetivo facilitar los 
  resultados arrojados a una solicitud. Query en bases de datos SQL y NoSQL es una técnica para obtener esos datos que se planea analizar. Por ejemplo, podrás pedir todos los datos del usuario y, además, podrás seleccionar la tabla o pedir 
  una hoja específica donde se encuentren las direcciones.

- Asincrónia: La programación asíncrona nos da la capacidad de “diferir” la ejecución de una función a la espera de que se complete una operación, normalmente de I/O (red, disco duro, …), y así evitar bloquear la ejecución hasta que se haya 
  completado la tarea en cuestión. Esto es posible gracias a que las funciones son ciudadanos de primer nivel (first-class citizens) y pueden ser pasadas como argumentos de otras funciones tal cual lo haríamos con las variables.    
  Asynchronous means that the the Web Application could send and receive data from the Web Server without refreshing the page. This background process of sending and receiving data from the server along with updating different sections of a 
  web page defines Asynchronous property/feature of AJAX.      
    
- AJAX (Asynchronous JavaScript and XML): Asynchronous JavaScript and XML (AJAX) is a combination of web application development technologies that make web applications more responsive to user interaction. Whenever your users interact with a 
  web application, such as when they click buttons or checkmark boxes, the browser exchanges data with the remote server. Data exchange can cause pages to reload and interrupt the user experience. With AJAX, web applications can send and 
  receive data in the background so that only small portions of the page refresh as required. 
  AJAX uses JavaScript and XML to enable asynchronous calls when browsers and servers exchange data. Next, we explain how browsers traditionally exchange data and compare it to data exchange with AJAX.
  
  > Data exchange without AJAX: In a conventional model, the browser sends an HTTP request to the server side when the user performs an action. The web server receives and processes the request and sends the updated data to the browser. 
    Then, the browser refreshes the webpage with the new data. In this approach, the browser reloads the entire page even if the requested data consists of minor changes. Moreover, the browser might send frequent requests, which load the web 
    server software. 
  
  > Data exchange with AJAX: Instead of updating the whole page, AJAX uses a JavaScript function to create an XMLHttpRequest object on the browser. Then, it compiles the page information in XML format, which the XMLHttpRequest object sends 
    to the web server. The web server processes the request and responds with the requested data. Lastly, the browser updates the current screen with the latest data without refreshing the page. 
  
  > Why is AJAX more efficient?: Despite similarities in data exchange and information flow, AJAX is more efficient than conventional web requests. With AJAX, the browser only updates specific web content based on the requested data. It 
    doesn't make unnecessary refreshes on other content on the page. This makes AJAX applications faster and more responsive than conventional web applications. 

  Es un conjunto de técnicas de desarrollo que permiten que las aplicaciones web funcionen de forma asincrónica para procesar tareas en segundo plano. En consecuencia, cualquier app o web que emplee 
  AJAX puede enviar y recibir datos sin volver a cargar toda la página, evitando la interrupción de acciones realizadas por el usuario, añadiendo interactividad y dinamismo a nuestra aplicación. El objetivo principal de AJAX es hacer que los 
  sitios y las aplicaciones web sean más fáciles de usar, más rápidos y con mayor capacidad de respuesta. AJAX tiene ciertas ventajas indiscutibles como:
    > Facilita la navegación, ya que la información se actualiza continuamente y las interacciones entre el usuario y las páginas web se vuelven más rápidas.
    > Reduce la carga en el servidor, aumentando su velocidad y capacidad, porque no se genera la información de la página completa, sino de una sola parte que debe actualizarse.
    > Aumenta la interactividad porque los resultados de búsqueda aparecen inmediatamente, lo que optimiza el proceso de búsqueda y mejora la experiencia del usuario.

  El modelo práctico de uso de AJAX tiene los siguientes pasos:
    > Paso 1. En el navegador se crea una llamada de JavaScript que activa XMLHttpRequest.
    > Paso 2. En segundo plano el navegador web crea una solicitud HTTP que va al servidor.
    > Paso 3. El servidor recibe, recupera y manda los datos al navegador web.
    > Paso 4. Los datos se reciben por la web y aparecen en la página sin que se recargue.
    
  En el modelo tradicional se crea una solicitud HTTP que va al servidor. El servidor procesa la solicitud y carga la página HTML sin que el usuario interactúe con la aplicación web. En el caso de AJAX, JS permite que el usuario interactúe 
  con la aplicación web y modifique la página en segundo plano y solo sus partes necesarias. “En segundo plano” significa que mientras espera que se reciban los datos, el usuario puede hacer otras cosas en la página y optimizar este tiempo.
 
- Promesas (promise): Es un objeto de Javascript que representa un evento a futuro y permite representar y seguir el ciclo de vida de una tarea/operación (función). Es una acción asincrónica que se puede completar en algún momento y 
  producir un valor, y notificar cuando esto suceda. Una promesa cuenta con tres estados posibles: pending, fulfilled y rejected. Las promesas pueden ser resueltas o rechazadas.
  Podemos crear promesas a través de su constructor new Promise. Su sintaxis es algo compleja, ya que recibe una función por parámetro que a su vez recibe por parámetro las funciones de resolve y reject
  
 
- XML (Extensible Markup Language): es un lenguaje de marcado que define un conjunto de reglas para la codificación de documentos. El lenguaje de marcado es un conjunto de códigos que se pueden aplicar en el análisis de datos o la lectura de 
  textos creados por computadoras o personas. El lenguaje XML proporciona una plataforma para definir elementos para crear un formato y generar un lenguaje personalizado. XML admite el intercambio de información entre sistemas de 
  computación, como sitios web, bases de datos y aplicaciones de terceros. Las reglas predefinidas facilitan la transmisión de datos como archivos XML a través de cualquier red, ya que el destinatario puede usar esas reglas para leer los 
  datos de forma precisa y eficiente.
  Un archivo XML se divide en dos partes: prolog y body. La parte prolog consiste en metadatos administrativos, como declaración XML, instrucción de procesamiento opcional, declaración de tipo de documento y comentarios. La parte del body se 
  compone de dos partes: estructural y de contenido (presente en los textos simples). El diseño XML se centra en la simplicidad, la generalidad y la facilidad de uso y, por lo tanto, se utiliza para varios servicios web. Tanto es así que hay 
  sistemas  destinados a ayudar en la definición de lenguajes basados ​​en XML, así como APIs que ayudan en el procesamiento de datos XML - que no deben confundirse con HTML.

- Protocolo: Conjunto de estándares y normas que deben seguirse para poder llevar a cabo una comunicación correcta
    
- Servidor: Sistema que permite recibir peticiones de otras computadoras y devolver una respuesta a éstas.
  Es un equipo informático que forma parte de una red y provee servicios a otros equipos. Es un aparato informático que almacena, distribuye y suministra información. Los servidores funcionan basándose en el modelo “cliente-servidor”. 
  El cliente puede ser tanto un ordenador como una aplicación que requiere información del servidor para funcionar. Por tanto, un servidor ofrecerá la información demandada por el cliente siempre y cuando el cliente esté autorizado. Los 
  servidores pueden ser físicos o virtuales.
  > Servidor proxy: realiza un cierto tipo de funciones a nombre de otros clientes en la red para aumentar el funcionamiento de ciertas operaciones (p. ej., prefetching y depositar documentos u otros datos que se soliciten muy 
    frecuentemente), también proporciona servicios de seguridad, o sea, incluye un cortafuegos.
  > Servidor del acceso remoto (RAS): controla las líneas de módem de los monitores u otros canales de comunicación de la red para que las peticiones conecten con la red de una posición remota, responde llamadas telefónicas entrantes y 
    reconoce la petición de la red.
  > Servidor web: almacena documentos HTML, imágenes, archivos de texto, escrituras, y demás material Web compuesto por datos (conocidos colectivamente como contenido), y distribuye este contenido a clientes que la piden en la red.
  > Servidor de base de datos: provee servicios de base de datos a otros programas u otras computadoras, como es definido por el modelo cliente-servidor. También puede hacer referencia a aquellas computadoras (servidores) dedicadas a 
    ejecutar esos programas, prestando el servicio.
     
- codigo de estado: Cuando realizamos alguna petición al servidor mediante el protocolo HTTP, el servidor debe respondernos no sólo con información, sino con un estado del proceso. Este es un código que nos permitirá saber cómo se encuentra 
  el proceso, o cómo finalizó.    
  
- HTTP: Acrónimo para Hyper Text Transfer Protocol. Es el protocolo que nos permite comunicarnos a través de Internet  

- HTTPS

- TSL

- SSL
  
- DNS: 

- HTTP response status codes: HTTP response status codes indicate whether a specific HTTP request has been successfully completed. Cuando el servidor responde con un código de estado, esto permite saber qué ocurrió con la consulta que 
  estábamos haciendo, y da información al cliente sobre qué ha ocurrido.
  ✓ 1xx: Status “informativo”
  ✓ 2xx: Status “ok”. Indica que la petición se procesó correctamente. No hubo ningún tipo de inconveniente desde la consulta hasta la respuesta.
  ✓ 3xx: Status de redirección. Hace referencia a redirecciones, cuando un recurso se ha movido o necesitamos apuntar a otro servicio.
  ✓ 4xx: Status de error de cliente. Se utiliza cuando el cliente realiza alguna petición que no cumpla con las reglas de comunicación (una mala consulta, tal vez le faltó enviar un dato, o venía en un formato incorrecto).
  ✓ 5xx: Status de error en servidor.
  
  ~ 200: Indica que la petición se procesó correctamente. No hubo ningún tipo de inconveniente desde la consulta hasta la respuesta
  ~ 300: Hace referencia a redirecciones, cuando un recurso se ha movido o necesitamos apuntar a otro servicio.
  ~ 400: Se utiliza cuando el cliente realiza alguna petición que no cumpla con las reglas de comunicación (una mala consulta, tal vez le faltó enviar un dato, o venía en un formato incorrecto).
  ~ 401: Se utiliza cuando el cliente no se ha identificado con el servidor bajo alguna credencial, no puede acceder al recurso
  ~ 404: Se utiliza cuando el recurso no se ha encontrado, ya sea algún dato solicitado o incluso el endpoint mismo.
  ~ 500: Se utiliza cuando algo ocurrió en el servidor, no necesariamente un error del cliente, sino un error o “detalle” que no haya considerado el servidor al tratar con algún caso
    
- Cookies (definicion): Cookies (often known as internet cookies) are text files with small pieces of data — like a username and password — that are used to identify your computer as you use a network. Specific cookies are used to identify 
  specific users and improve their web browsing experience. Data stored in a cookie is created by the server upon your connection. This data is labeled with an ID unique to you and your computer. When the cookie is exchanged between your
  computer and the network server, the server reads the ID and knows what information to specifically serve you.
  Due to international laws, such as the EU’s General Data Protection Regulation (GDPR), and certain state laws, like the California Consumer Privacy Act (CCPA), many websites are now required to ask for permission to use certain cookies
  with your browser and provide you with information on how their cookies will be used if you accept.
  
  Type of cookies: 
  1. Magic cookies Magic cookies were originally used by Unix programmers to authenticate and track users in a system. Magic cookies are data tokens that allow servers and web browsers to communicate.
     HTTP cookies are a type of magic cookie used by websites to store information. The data stored in magic cookies are encrypted and, under normal circumstances, only the server that created the cookie can read the data.
  2. HTTP Cookies:  HTTP cookies, or internet cookies, are built specifically for web browsers to track, personalize and save information about each user’s session. A “session” is the word used to define the amount of time you spend on a 
     site. Cookies are created to identify you when you visit a new website. The web server — which stores the website’s data — sends a short stream of identifying information to your web browser in the form of cookies. This identifying 
     data (known sometimes as “browser cookies”) is processed and read by “name-value” pairs. These pairs tell the cookies where to be sent and what data to recall. So, where are the cookies are stored? It’s simple: your web browser will 
     store them locally to remember the “name-value pair” that identifies you. When you return to the website in the future, your web browser returns that cookie data to the website’s server, triggering the recall of your data from your 
     previous sessions. To put it simply, cookies are a bit like getting a ticket for a coat check:
     ~ You hand over your “coat” to the cloak desk. You connect/visit a website and a pocket of data is linked to you on the website’s server. This data can be your personal account, your shopping cart or even just what pages you’ve visited.
     ~ You get a “ticket” to identify you as the “coat” owner. The cookie (containing the data) is then given to you and stored in your web browser. It has a unique ID especially for you.
     ~ If you leave and return, you can get the “coat” with your “ticket”. When you revisit the website, your browser gives the website the cookie back. The website then reads the unique ID in the cookie to assemble your activity data,
       bringing you back to where you were when you first visited, as if you never left.
  3. First-party cookies: First-party cookies are from websites you directly visit in your browser and are used to improve your online user experience. They often store information relevant to the website such as what you’ve viewed in the 
    past or your settings preferences. As long as you are visiting authentic and reputable websites, first-party cookies are usually harmless and make it easier to browse your favorite websites.
  4. Third-party cookies: Third-party cookies are probably the most controversial type of cookie in terms of data privacy. They usually track your behavior for advertising purposes and aren't a direct part of the websites you visit. Instead, 
     they’re usually embedded in ads, videos, or web banners. Even a Facebook "like" button uses third-party cookies.
  5. Zombie cookies: Also known as supercookies, zombie cookies are a type of third-party cookie. However, they aren't stored in the same place as regular cookies. So even if a person deletes cookies, zombie cookies will rise from the dead 
     and reinstall themselves. They have gained a reputation for being notoriously difficult to remove.
  6. Session cookies:Session cookies work by storing information while you're browsing a website. This means it won't have to reauthenticate you for every web page you visit. Once you exit, your browser deletes all session cookies.
     Session cookies enable you to add an item to your shopping cart, browse multiple other pages, and then still keep track of your item in your cart. These are one of the most common types of cookies. 
  7. Persistent cookiesL: Persistent cookies are used to track and collect information about you. This particular cookie enables websites to remember if you're logged in and under what account. It's also used to build a profile on your 
     search history, so websites can recommend products, services, or content relevant to you. Most of these cookies usually have an expiration date. Persistent cookies are also a common type of cookie.
  8. Essential cookies: You're probably familiar with the banner or pop-up asking you for your cookie preferences for a website. Essential cookies are frequently an option to run only cookies necessary to run the website or for services you 
     have requested (such as remembering your login credentials). This means you remove third-party cookies from your website experience.
  9. Performance cookies: As the name suggests, performance cookies track your online movements and that data is used to improve the website. They measure analytics like how many times you visited a page, how much time you spent on a page, 
     or when you left the website. This is often a first-party cookie, but many websites use a third party to track these analytics.
  10. Functionality cookies: Functionality cookies allow you to use the fundamental features of a website. This could be anything from your language preference to displaying local news stories. They typically enhance a website's performance 
      and functionality. Some site features may not be available without functional cookies.    
  11. Advertising cookies: Third-party persistent cookies are often used for advertising purposes. Advertising cookies (also called targeting cookies) build a profile on you based on your interests, search history, and items you view. They 
      then share that information with other websites, so they can advertise relevant products and services to you.
    For example, maybe you searched for gym shoes recently. Don't be too surprised later when you see an ad on social media for gym shoes or relevant items such as socks.

      
   > What Are Cookies Used For: Websites use HTTP cookies to streamline your web experiences. Without cookies, you’d have to login every time you leave a site or rebuild your shopping cart if you accidentally closed the page. Making cookies 
     is an important part of the modern internet experience. To be more concise, cookies are intended to be used for:
      1. Session management: For example, cookies let websites recognize users and recall their individual login information and preferences, such as sports news versus politics.
      2. Personalization: Customized advertising is the main way cookies are used to personalize your sessions. You may view certain items or parts of a site, and cookies use this data to help build targeted ads that you might enjoy. They’re 
         also used for language preferences as well.
      3. Tracking: Shopping sites use cookies to track items users previously viewed, allowing the sites to suggest other goods they might like and keep items in shopping carts while they continue shopping on another part of the website. \
         They will also track and monitor performance analytics, like how many times you visited a page or how much time you spent on a page.
         
      While this is mostly for your benefit, web developers get a lot out of this set-up as well. Cookies are stored on your device locally to free up storage space on a website’s servers. In turn, websites can personalize content, whilst 
      saving money on server maintenance and storage costs.
      
   > What are the different types of HTTP Cookies: With a few variations (which we’ll discuss later), cookies in the cyber world essentially come in two types: session cookies and persistent cookies.
     Session cookies are used only while navigating a website. They are stored in random access memory and are never written on to the hard drive. When the session ends, session cookies are automatically deleted. They also help the "back"
     button work on your browser.
     Persistent cookies, on the other hand, remain on a computer indefinitely, although many include an expiration date and are automatically removed when that date is reached. Persistent cookies are used for two primary purposes:
      ~ Authentication. These cookies track whether a user is logged in and under what name. They also streamline login information, so users don't have to remember site passwords.
      ~ Tracking. These cookies track multiple visits to the same site over time. Some online merchants, for example, use cookies to track visits from particular users, including the pages and products viewed. The information they gain 
        allows them to suggest other items that might interest visitors. Gradually, a profile is built based on a user's browsing history on that site.      
  
  
- Application logic vs Business logic: 
  
- software architecture patterns: If you design software architectures, chances are that you come across the same goals and problems over and over again. Architectural patterns make it easier to solve these issues by providing repeatable 
  designs that address common situations.
 
   1) The circuit breaker pattern minimizes the effects of a hazard by rerouting traffic to another service. While it helps make systems more fault tolerant to prevent accidents, it also requires sophisticated testing and using an 
      infrastructure-management technology like service mesh.
   2) The client-server pattern is a peer-to-peer architecture that is comprised of a client, which requests a service, and a server, which provides the the service. Examples include banking, file sharing, email, and the World Wide Web. One 
      advantage of this pattern is that data and network peripherals are centrally managed, however, the server is expensive.
   3) The command query responsibility segregation (CQRS) pattern handles the situation where database queries happen more often than the data changes. It separates read and write activities to provide greater stability, scalability, and 
      performance, but it requires more database technologies and therefore may increase costs.
   4) The controller-responder pattern divides the architecture into two components: The controller handles the data and distributes workloads, and the responder replicates data from the controller and generates results. One advantage is 
      that you can read data from the responder without affecting the data in the controller, but if the controller fails, you may lose data and need to restart the application.
   5) The event sourcing pattern is good for applications that use real-time data. It sends a continuous stream of messages to a database, web server, log, or another target. It's very flexible but demands a highly efficient and reliable 
      network infrastructure to minimize latency.
   6) The layered pattern is good for e-commerce, desktop, and other applications that include groups of subtasks that execute in a specific order. The layered pattern makes it easy to write applications quickly, but a disadvantage is that 
      it can be hard to split up the layers later.
   7) The microservices pattern combines design patterns to create multiple services that work interdependently to create a larger application. Because each application is small, it's easier to update them when needed, but the complexity 
      means you need greater architectural expertise to make everything work correctly.
   8) The model-view-controller (MVC) pattern divides an application into three components. The model contains the application's data and main functionality; the view displays data and interacts with the user; and the controller handles user 
      input and acts as the mediator between the model and the view. This pattern enables the application to generate various views, but its layers of abstraction increase complexity.
   9) The pub-sub pattern sends (publishes) relevant messages to places that have subscribed to a topic. It's easy to configure but more challenging to test because interactions between the publisher and the subscriber are asynchoronous.
   10) The saga pattern is used for transactions with multiple steps, such as travel reservation services. A "saga" includes the various steps that must happen for the transaction to complete. This pattern enables transactions (ideally with 
       five or fewer steps) to happen in loosely coupled, message-driven environments, but it requires a lot of programming and can be complex to manage.
   11) The sharding pattern segments data in a database to speed commands or queries. It ensures storage is consumed equally across instances but demands a skilled and experienced database administrator to manage sharding effectively.
   12) The static content hosting pattern is used to optimize webpage loading time. It stores static content (information that doesn't change often, like an author's bio or an MP3 file) separately from dynamic content (like stock prices). 
       It's very efficient for delivering content and media that doesn't change often, but downsides include data consistency and higher storage costs.
   13) The strangler pattern is used when you're making incremental changes to a system. It places the old system behind an intermediary to support incremental transformation, which reduces risk compared to making larger changes. However, 
       you need to pay close attention to routing and network management and make sure you have a rollback plan in place in case things go wrong.
   14) The throttling (or rate-limiting) pattern controls how fast data flows into a target. It's often used to prevent failure during a distributed denial of service attack or to manage cloud infrastructure costs. To use this pattern 
       successfully, you need good redundancy mechanisms in place, and it's often used alongside the circuit breaker pattern to maintain service performance.

- Los principios SOLID: The SOLID principles of software architecture consist of a collection of guidelines that can help programmers build better software. These principles help developers build loosely coupled, cohesive systems that have 
  high cohesion and low coupling.
    > The Single Responsibility Principle (S): Per the Single Responsibility Principle, every class should not have more than one responsibility, (i.e., it should have one and only one purpose). If you have multiple responsibilities, the 
      functionality of the class should be split into multiple classes, with each of them handling a specific responsibility. Types with many responsibilities tend to be coupled with one another. This coupling can lead to fragile designs and 
      such classes become difficult to manage and maintain over time. If you adhere to this principle, here are the benefits of the Single Responsibility Principle:
       + Simplicity: The code is easier to understand since the functionality is not spread across multiple classes. This will help you keep your simple, manageable and clean.
       + Maintainability: This reduces the complexity and increases the maintainability of your code since each class has a single responsibility only.
       + Reusability: Since there are no dependencies between different parts of the system, you can reuse components across the application without worrying about breaking anything else.

    > The Open Closed Principle (O): According to the Open Closed Principle, classes should be open for extension, (i.e., they can be extended but closed for modification and they should not be modifiable). When classes are open for 
      extension but closed for modification, developers can extend the functionality of a class without having to modify the existing code in that class. In other words, programmers should make sure their code can handle new requirements 
      without compromising on the existing functionality. Bertrand Meyer is credited with introducing this principle in his book entitled “Object-Oriented Software Construction.” According to Meyer, “a software entity should be open for 
      extension but closed for modification.”
      The idea behind this principle is that it allows developers to extend software functionality while preserving the existing functionality. In practical terms, this means that new functionality should be added by extending the code of an 
      existing class rather than by modifying the code of that class. When code is extended rather than modified, there is less risk of introducing bugs. It can also make it easier to understand code since the structure of classes is not 
      changed when new functionality is added. Extending classes is not always possible or desirable, however. In some cases, creating a new class with the required functionality may be better, rather than extending an existing class.
      Here are the benefits of the Open Closed Principle at a glance:
       + You can add new features without changing existing code
       + Your application will be more flexible because it can evolve over time
       + It reduces the time and effort required to add new features to an application
       + It increases the maintainability of the source code

     > Liskov Substitution Principle (L): The Liskov Substitution Principle, or LSP, is a design principle that states that replaceable and interchangeable types should behave similarly. The principle, which Barbara Liskov introduced in her 
       1988 paper, “Data Abstraction and Hierarchy,” states that, if you have a type T and a subtype S of T, then objects of type S should be substitutable for objects of type T.
       It follows that if B is a subtype of A, then objects of type B can be used as substitutes for objects of type A. In other words, if you have a class A and a class B, with B being a subclass of A, then you can replace any instance of B 
       with an instance of A.
       It states that a child class should be able to be used in place of a parent class without any errors. This principle is essential for ensuring that software components are interchangeable and can be easily replaced without affecting 
       the rest of the code.
     
     > The Interface Segregation Principle (I): The Interface Segregation Principle is a design principle that says you should “write client-specific interfaces, and make sure clients don’t depend on methods of other interfaces.” This means 
       that, if you want to use an alternative implementation, you can do so without having to change any client code. In other words, an interface should be designed so that clients only have to know about the methods they need to use. This 
       principle is fundamental in object-oriented programming (OOP), where interfaces are used to define the contracts between objects. Adhering to the Interface Segregation Principle can make a developer’s code more flexible and 
       maintainable. This helps to prevent tight coupling between objects, which makes them easier to reuse and maintain. Here are the benefits of the Interface Segregation Principle at a glance:
        + Reduces coupling between components because they don’t share the same interface
        + Encourages loose coupling between components, which makes them easier to change, maintain and testable
        + Allows components to be replaced with alternative implementations
      
      > Dependency Inversion Principle (D): Per the Dependency Inversion Principle, high-level modules in an application should not rely on their low-level modules. Instead, both should rely on abstractions. While details should depend on 
        abstractions, the reverse is not implied. The Dependency Inversion Principle recommends abstractions over concretions. Here are several benefits to the Dependency Inversion Principle:
         + It makes code more flexible, reusable, modular, and easier to change
         + It makes code more testable since high-level modules can be mocked or stubbed out when testing low-level modules
         + It can make code more flexible since new low-level modules can be easily plugged in without having to make changes to high-level modules.
       One way to achieve dependency inversion is through the use of abstractions. Abstractions can be created using interfaces or abstract base classes. By depending on abstraction instead of a concrete implementation, high-level modules 
       can be easily changed to use different implementations without making any changes.
       Developers can also achieve dependency inversion by leveraging inversion of control containers. These containers manage the creation and lifetime of objects and provide a mechanism for resolving dependencies. Using an inversion of 
       control container allows high-level modules to be easily tested without worrying about dependencies. However, dependency inversion is not always easy to implement.

  These principles can help you build resilient, maintainable, and extendable applications. Some of the benefits of adhering to the solid principles of software architecture include:
   ~ More robust systems: By following solid principles, developers can create systems that are more resistant to change and less likely to break when modifications are made.
   ~ Better Reusability: By adhering to these principles, you can build reusable components.
   ~ Easier maintenance: Solid principle-based systems are typically easier to maintain and understand, making them less time-consuming and expensive to keep up-to-date.
   ~ Better scalability: Another advantage of using solid principles is that systems designed this way are often more scalable, meaning they can be extended over time if needed.

- Interpolation: 

- Proxy: Un proxy es un equipo informático que hace de intermediario entre las conexiones de un cliente y un servidor de destino, filtrando todos los paquetes entre ambos. Siendo tú el cliente, esto quiere decir que el proxy recibe tus 
  peticiones de acceder a una u otra página, y se encarga de transmitírselas al servidor de la web para que esta no sepa que lo estás haciendo tú. De esta manera, cuando vayas a visitar una página web, en vez de establecer una conexión 
  directa entre tu navegador y ella puedes dar un rodeo y enviar y recibir los datos a través de esta proxy. La página que visites no sabrá tu IP sino la del proxy, y podrás hacerte pasar por un internauta de otro país distinto al tuyo.
  Los proxys son utilizados muy a menudo para acceder a servicios que tienen bloqueado su contenido en determinado país. Por ejemplo, si una web no ofrece determinado contenido en tu país pero sí en otro, haciéndote pasar por un internauta
  de ese otro país puedes acceder a él. Como muchos de estos servicios de proxy bloquean también cookies, scripts y otros objetos que están alojados en las webs, también son útiles para poder navegar de una manera mucho más privada y anónima
   > ¿Qué es un servidor proxy? Un servidor proxy proporciona una puerta de enlace entre los usuarios e Internet. Es un servidor denominado “intermediario”, porque está entre los usuarios finales y las páginas web que visitan en línea.
     Cuando una computadora se conecta a Internet, utiliza una dirección IP. Esto es similar a la dirección de su casa: le indica a los datos entrantes adónde ir y marca los datos salientes con una dirección de devolución para que otros 
     dispositivos se autentiquen. Un servidor proxy es esencialmente una computadora en Internet que tiene una dirección IP propia.
   > Servidores proxy y seguridad de red: Los servidores proxy proporcionan una valiosa capa de seguridad para su computadora. Pueden configurarse como filtros web o firewalls, y protegen a su computadora contra amenazas de Internet como el 
     malware. Esta seguridad adicional también es valiosa cuando se combina con una puerta de enlace web segura o con otros productos de seguridad de correo electrónico. De esta manera, puede filtrar el tráfico de acuerdo con su nivel de 
     seguridad o la cantidad de tráfico que su red, o las computadoras individuales, pueden manejar. Algunas personas usan los proxies para fines personales, como ocultar su ubicación mientras miran películas en línea. Sin embargo, para una 
     compañía, pueden utilizarse para realizar varias tareas clave como las siguientes: 
       1) Mejorar la seguridad.
       2) Proteger la actividad de los empleados en Internet de las personas que intentan espiarlas.
       3) Equilibrar el tráfico de Internet para evitar choques.
       4) Controlar el acceso de los empleados a los sitios web.
       5) Guardar el ancho de banda almacenando archivos en caché o comprimiendo el tráfico entrante.
   > Cómo funciona un proxy: Debido a que un servidor proxy tiene su propia dirección IP, actúa como un mediador para una computadora e Internet. Su computadora conoce esta dirección y, cuando usted envía una solicitud en Internet, se enruta 
     al proxy, que luego obtiene la respuesta del servidor web y reenvía los datos de la página al navegador de su computadora, como Chrome, Safari, Firefox o Microsoft Edge.
   > ¿Cuáles son los diferentes tipos de servidores proxy? Existe una amplia variedad de proxys, a continuación, veremos cuáles son los más comunes.
     1) Proxy web: Este es el tipo de proxy más habitual, al que todos los usuarios podemos acceder a través de una página web. De hecho, la web actúa como proxy. Se basa en HTTP y HTTPS actuando como intermediario para acceder a otros 
        servicios en Internet. Toda nuestra navegación pasará por el proxy web que estemos utilizando.
     2) Proxy caché: Este servidor es un intermediario entre la red a la que nos conectamos e Internet, para registrar el contenido antes: HTML, CSS e imágenes. Es muy utilizado para acelerar el contenido de un sitio al navegar.
        Los datos de una web quedan almacenados en la primera visita y si hay una segunda no necesita revisarlos todos de nuevo. Así el acceso es mucho más rápido.
     3) Proxy inverso: Un proxy inverso es un servidor que acepta todo el tráfico y lo reenvía a un recurso específico. Este tipo de proxy aporta seguridad al servidor principal, restringiendo el acceso a rutas definidas. Esto permite, entre 
        otras funciones, evitar ataques.   
     4) Proxy transparente: Para utilizar este servidor proxy el usuario no tendrá que configurar nada antes de comenzar la navegación. Actúa como un intermediario entre nuestro equipo e Internet pero sin modificar nada.
     5) Proxy NAT: Este proxy es capaz de ocultar la identidad de los usuarios, enmascarando la auténtica dirección IP. Cuenta con una amplia variedad de configuraciones.
      
- Router: Es un dispositivo de conexión en la red que sirve para encontrar la ruta más corta al entregar los paquetes de datos. Su principal propósito es conectar diferentes redes de manera simultánea. Al igual que el firewall, funciona en 
  la capa de red, pero también funciona en la capa física y la capa de enlace de datos del modelo OSI. A diferencia del firewall, no incluye cifrado, por lo tanto, no protege la red ante amenazas aunque comparta la conexión a Internet entre 
  las redes y los equipos.
  
- firewall: Es un sistema entre dos redes que implementa una política de control de acceso entre estas. Es decir, todos los paquetes de datos que ingresan o abandonan la red pasarán a través del firewall, y verifica si permite su paso o no. 
  También funciona en la capa de red del modelo OSI, pero a diferencia del router, cifra los datos antes de su transmisión. Un firewall principalmente protegerá la red ante amenazas. Puede implementarse tanto en hardware como en software.

- Compresion: Cuando el servidor se está comunicando con el cliente en el navegador, parte de esta comunicación implica revisar si hay algún archivo comprimido que necesite descomprimirse, y en caso de que así sea, cuál sería el algoritmo de 
  descompresión a utilizar para poder obtener la información correctamente. Los navegadores modernos pueden aceptar contenido codificado en tres algoritmos principales: 
    ✓ Deflate: 
    ✓ Gzip (Deflate + algunas cosas adicionales): Gzip es el primer y más conocido modelo de compresión, es altamente utilizado y sencillo de utilizar. La compresión se puede colocar a nivel middleware en nuestro servidor para poder 
      corroborar la diferencia de transferencia ntre una respuesta con y sin compresión.
    ✓ Brotli: Brotli es conocido como una alternativa “moderna” de Gzip. Éste fue desarrollado por Google y ofrece un algoritmo cuya compresión puede resultar hasta 30% más efectiva que la compresión de Gzip . 


- Escalamiento vertical: Mi servidor necesita ser más potente y necesito mejorar el hardware para tener un servidor más potente. Básicamente, significa mejorar el hardware del servidor, para que sea más potente, mucho más rápido y pueda 
  atender una mayor cantidad de peticiones y, por lo tanto, mejorar el performance de los aplicativos. El escalamiento vertical requiere de grandes inversiones de recursos por parte de las empresas para poder contar con los equipos más 
  actualizados posibles en el mundo de la tecnología. Además, llegará un punto en el que alcanzaremos un tope tecnológico, y tendremos que esperar a que se desarrollen mejores soluciones de hardware para poder comprarlas (un tiempo de espera 
  que una empresa difícilmente puede contener).

- Escalamiento horizontal: Dividamos las tareas en multi-instancias de servidores que alojen el aplicativo y se apoyen en las tareas complejas. Este modelo es más complejo, pero mucho más interesante y eficiente. La escalabilidad horizontal 
  significa utilizar múltiples servidores, conocidos como nodos, los cuales trabajarán en equipo para resolver un problema en particular. A esta red de nodos trabajando juntos, se le conoce como cluster, haciendo referencia a que estos 
  múltiples servidores se encuentran en un contexto general donde todos conocen cómo ayudarse a las tareas más complejas. Así, la diferencia radica en que, cuando necesitamos más recursos, no hace falta tirar el servidor que ya tenemos a la 
  basura para comprar uno mejor, sino que podemos conectar otra instancia de otro servidor para que se una a la red de nodos y forme parte del cluster.

- Cluster: Es un módulo nativo de node js que nos permitirá ejecutar este concepto de clusterización que recién comentamos, donde podremos tener a un proceso principal contando con un grupo de procesos trabajadores. Estos trabajadores van a 
  trabajar en conjunto para resolver el problema de las situaciones de las peticiones.
  
- Maquina virtual: 

- Kernel: El Kernel o núcleo, es una parte fundamental del sistema operativo que se encarga de conceder el acceso al hardware de forma segura para todo el software que lo solicita, el Kernel es una pequeña e invisible parte del sistema 
  operativo, pero la más importante, ya que sin esta no podría funcionar. Todos los sistemas operativos tienen un Kernel, incluso Windows 10, pero quizá el más famoso es el Kernel de Linux. Este núcleo de los sistemas operativos se ejecuta 
  en modo privilegiado con acceso especial a los recursos del sistema para poder realizar las peticiones de acceso que le va pidiendo el software que lo necesita, además como los recursos no son ilimitados, también hace de arbitro a la hora 
  de asignarlos, decidiendo el orden de las peticiones recibidas según la prioridad e importancia de estas. Una gestión muy importante y fundamental que en la mayoría de las ocasiones pasa desapercibida aún siendo un trabajo esencial para 
  coordinar todo el hardware con el software.
  
  El Kernel o núcleo de un sistema operativo sirve para administrar los recursos de hardware solicitados por los diferentes elementos de software y hacer de intermediario decidiendo a que y cuando se concede este acceso evitando así 
  sobrecarga del sistema, recursos innecesarios y acceso a software malicioso al propio Kernel y llegar a poder controlar así todo el sistema. De este modo el Kernel sirve como elemento de seguridad teniendo que pasar por varias capas antes 
  de poder tener acceso, además tiene que distribuir los recursos de manera eficiente y ordenada para que el Hardware trabaje junto al Software de la mejor manera posible.
  Aunque usualmente relacionamos un Kernel o un núcleo del sistema operativo a un PC, también está presente y sirve para hacer funcionar todos los computadores que podemos encontrar hoy en día, como por ejemplo un ordenador de a bordo de un 
  coche o un barco, una raspberry PI que ejecuta una versión adaptada de Linux Debian o los dispositivos móviles con Android e iOS, que  también disponen de un Kernel basado en Linux / Unix.
  Tiene también como trabajo conceder acceso a todos los periféricos que tengamos conectados e interactuar con el software que los solicite, aunque no sean los usuales con los que trabajamos. Por ejemplo si ocasionalmente conectamos un móvil 
  para usarlo como webcam con DroidCam, este Kernel se encarga de conceder los permisos necesarios al software para gestionar y poder tener la imagen y el audio para poder usarla en algún software de videoconferencia o reuniones si por 
  ejemplo teletrabajamos desde casa o cualquier otro lugar.       --> https://salesystems.es/que-es-un-kernel/#:~:text=El%20kernel%20es%20el%20componente,o%20núcleo%20del%20sistema%20operativo.
  El Kernel es el encargado de hacer funcionar básicamente todo, tiene que ser capaz de arrancar, por ejemplo, un PC desde que lo encendemos hasta que vemos visible el escritorio, todo esto comunicándose con los elementos hardware que 
  dispone el PC y que también son necesarios para hacerlo funcionar, una vez que tengamos el escritorio deberá ser capaz de hacer funcionar los programas que nosotros queramos abrir y hacerlos funcionar en nuestro PC.
  
- Docker: Docker es una plataforma gestora de contenedores. Nos permitirá entonces empaquetar en un contenedor nuestro aplicativo, y posteriormente compartirlo a algún lado, para que al momento en el que tenga que ejecutarse, este pueda 
  hacerlo dentro del contenedor aislado y asegurar que la ejecución será satisfactoria siempre. La lógica de Docker se basa en tres pasos generales: 
    1) Un dockerfile: Este cuenta con las instrucciones paso a paso para que nuestro proyecto genere una imagen
    2) Una imagen es el equivalente de una clase, pero con un proyecto completo. Cuando generamos la imagen de una aplicación, significa que podemos generar múltiples contenedores a partir de esa aplicación (como instancias) 
    3) Contenedor: El punto final en el que ejecutamos el aplicativo, pero esta vez desde un entorno cerrado.  
    
- DockerHub: Dockerhub es una librería, o repositorio de imágenes en la nube. Cuando hemos finalizado con el mantenimiento de nuestro aplicativo, necesitamos compartir la imagen resultante con nuestro equipo (Y enviar la imagen por correo 
  electrónico no resulta óptimo). Al subir nuestra imagen en la nube, todos los miembros autorizados podrán descargarla y utilizarla. Existen múltiples razones para tener nuestra imagen en un repositorio (nube):
    > Podemos compartir nuestra imagen con nuestro equipo de desarrollo.
    > Tenemos un sistema mejor controlado de nuestra imagen, ya que cada cambio de ésta puede significar una nueva tag con otra versión.
    > Permite que otros softwares descarguen la imagen La razón de que éste se encuentre en la nube y permita que otros softwares lo descarguen, significa que al momento de configurar procesos más complejos (como deployment), podemos obtener 
      la imagen directamente, sin necesidad de tener que estar cambiando el archivo local. 

- Imagen base: Escribimos en nuestro código --> FROM node. Esto significa que estaremos tomand una imagen base del entorno de node, para poder configurar nuestra app. 

- Contenedor: Un contenedor es un entorno de ejecución para un aplicativo en particular, el cual tiene todas las dependencias que necesita dicha aplicación para poder correr sin problemas de compatibilidad. La clave de un contenedor es el 
  concepto del aislamiento, esto indicando que podemos tener múltiples contenedores, con diferentes entornos, con diferentes dependencias, y nunca habrá conflictos porque la instalación y uso de las dependencias se hace de manera interna. 
  Además, ya que el entorno no ocupa utilizar todo el sistema operativo (sólo el kernel), se vuelven realmente livianos en comparación con mover todo un sistema operativo en cada aplicativo.

- Clusterización: 
  
- Orquestación: Es un término muy similar a la clusterización (a veces tomados por iguales), sin embargo, en esta clase haremos una ligera distinción. Al realizar una clusterización, estamos instanciando un modelo Primary-worker, en la cual 
  los procesos workers pueden reiniciarse una vez caídos. ¿Y la orquestación? también se trata de tener un proceso principal, el cual se encargue de cargar con workers, sólo que ésta vez cada worker será en esencia un contenedor por sí solo.
  La diferencia entre este termino y el anterior es que en que la orquestación es un proceso más profundo, ya que no sólo se trata de hacer la división, sino que permite realizar la gestión de éstos de manera más controlada, permitiendo
    ✓ Control de reinicio para contenedores que presenten algún fallo
    ✓ Balanceador de carga, el cual permitirá que las peticiones puedan llegar a los contenedores de manera más distribuida, a diferencia de en cola (como lo hace la clusterización base).
    ✓ Manejo de rollout updates, los cuales nos permiten seguir un flujo organizado para poder actualizar o reconfigurar contenedores en un cluster.
    
- Kubernetes: Kubernetes es una tecnología de orquestamiento de contenedores. En esencia, se trata de una plataforma que sirve para administrar cargas de trabajos y servicios. Kubernetes tomará un conjunto de instrucciones y las ejecutará 
  para poder distribuir pods, donde cada pod puede tener n contenedores, de esta forma todos los contenedores que pertenezcan a un pod, podrán funcionar como una entidad única para intercomunicarse. Kubernetes está pensado para poder ser 
  desplegado, eso quiere decir que podemos hacerlo desde alguna plataforma en la nube, aunque en este caso, por cuestión de costos, lo utilizaremos de manera local. Hay múltiples posibilidades para poder correr un repositorio local:
    ✓ Minikube
    ✓ MicroK8s
    ✓ k3s
    
- Minikube: Es un software que permite levantar un cluster local de kubernetes, permitiendo hacer las pruebas que necesitemos dentro de nuestro contenedor de docker. Para poder ejecutar el instalador, bastará con descargarlo de este link 
  (https://minikube.sigs.k8s.io/docs/start/). No hace falta gran configuración, todo se instalará directamente en la computadora. Para comenzar a trabajar con Minikube, bastará con corre el comando "minikube start".
  Recuerda haber instalado previamente kubectl, ya que al momento de iniciar minikube, buscará si está instalado, de otra forma lo tendremos que configurar manual.

- Ancho de banda: Teoricamente, es la longitud de la extensión de frecuencias, medida en hercios (Hz), en la que se concentra la mayor potencia de la señal.
  Es la máxima cantidad de datos transmitidos a través de una conexión a internet en cierta cantidad de tiempo. El ancho de banda por lo general se confunde con la velocidad de internet cuando en realidad es el volumen de información que se 
  puede enviar a través de una conexión en una cantidad medida de megabits por segundo (Mbps).
  Tambien se conoce como la cantidad máxima de datos que pueden ser transmitidos a través de una conexión a Internet en un tiempo determinado. Normalmente, el ancho de banda se representa en el número de bits, kilobits, megabits o 
  gigabits que se pueden transmitir en 1 segundo.
  Los términos velocidad de internet y ancho de banda suelen confundirse, pero aquí encontrarás la diferencia. La velocidad se refiere al ritmo al que se pueden transmitir los datos, mientras que el ancho de banda es la capacidad de esa 
  velocidad. Utilizando una analogía, imagina una carretera, la velocidad dependerá del vehículo y el ancho de banda de cuantos vehículos pueden transitar al mismo tiempo por ella durante un tiempo determinado.
  
- Velocidad: Es la velocidad es cuán rápido esa información se recibe o descarga. Tambien se define como la velocidad de dígitos binarios transmitidos y se da en bits por segundo (bps).

- Rendimiento: es la cantidad de información que se entrega con éxito en una determinada cantidad de tiempo.

- Latencia: La latencia a veces se refiere a una demora o tasa de ping. Es la demora que experimentas mientras esperas que algo se cargue. Si el ancho de banda es la cantidad de información enviada por segundo, la demora es la cantidad de 
  tiempo que demora esa información en ir de la fuente hasta ti.

- Transmicion: El rendimiento es cómo la información realmente se entrega en una determinada cantidad de tiempo. De modo que si el ancho de banda es la cantidad máxima de datos, el rendimiento es cuántos de esos datos llegan a su destino, 
  teniendo en cuenta la latencia, velocidad de red, pérdida de paquete y otros factores.

- Frecuencia: La frecuencia es la magnitud física que mide las veces por unidad de tiempo en que se repite un ciclo de una señal periódica. Una señal periódica de una sola frecuencia tiene un ancho de banda mínimo. En general, si la señal 
  periódica tiene componentes en varias frecuencias, su ancho de banda es mayor, y su variación temporal depende de sus componentes frecuenciales.

- Hertz: Los microprocesadores manejan velocidades de proceso de datos en el sistema, y eso se llama Hertz. Esta velocidad es la velocidad de reloj y a medida que va subiendo el nivel de velocidad, es mejor el rendimiento del microprocesador.
  Unidad de medida de la frecuencia electromagnética. Se utiliza para medir la velocidad de los procesadores. Equivale a un ciclo por segundo. En informática se utiliza para dar una idea de la velocidad del microprocesador, indicando cual es 
  la frecuencia de su clock (componente de los microprocesadores que genera una señal cuya frecuencia es utilizada para enmarcar el funcionamiento del procesador: a mayor frecuencia mayor velocidad).

- Frecuencia de refresco: representa el número de veces que se actualiza la imagen completa que vemos en la pantalla en cada segundo. Así, una frecuencia de 60 Hz significa que en realidad el monitor está mostrando 60 imágenes en un solo 
  segundo, si hablamos de 144 Hz entonces este lo hará 144 veces, uno de 240 Hz lo hará 240 veces. Si hablamos en términos comunes esto viene a significar lo suaves que son las transiciones de imágenes en la pantalla, ya que cuantas más 
  imágenes se generen por segundo, menos «saltos» de imagen veremos y más suave irá la acción del juego. Cuando hablamos de tasa de refresco nos estamos refiriendo a algo que es relativo al hardware (del monitor).

- Cache: Una memoria caché puede ser usada a veces como un búfer, y viceversa. Sin embargo, una caché opera con el supuesto de que los mismos datos van a ser utilizados múltiples veces, que los datos escritos serán leídos en un periodo corto    
  de tiempo, o teniendo en cuenta la posibilidad de múltiples lecturas o escrituras para formar un único bloque más grande. Su premisa básica es reducir los accesos a los almacenamientos de nivel más bajo, los cuales son bastante lentos. La 
  caché también es normalmente una capa de abstracción que está diseñada para ser invisible.
  Una caché de disco o archivo de caché guarda las estadísticas de los datos almacenados en él y proporciona datos con un tiempo máximo de espera en modos de escritura en diferido. Un búfer, por el contrario, no hace nada de esto, sino que es 
  utilizado normalmente en entrada, salida y a veces, almacenamiento temporal de datos que se enrutan entre distintos dispositivos o que van a ser modificados de manera no secuencial antes de ser escritos o leídos de manera secuencial.

- Buffer: Un buffer es un espacio en memoria en el que se almacenan datos de manera temporal. Su principal función es evitar que el programa o recurso que los requiere, ya sea hardware o software, se quede sin datos durante una transferencia 
  (entrada/salida) de datos irregular o por la velocidad del proceso. 
  Normalmente los datos se almacenan en un búfer mientras son transferidos desde un dispositivo de entrada (como un ratón o mouse) o justo antes de enviarlos a un dispositivo de salida (por ejemplo: altavoces). También puede utilizarse para 
  transferir datos entre procesos, de una forma parecida a los búferes utilizados en telecomunicaciones. Un ejemplo de esto último ocurre en una comunicación telefónica, en la que al realizar una llamada esta se almacena, se disminuye su 
  calidad y el número de bytes a ser transferidos, y luego se envían estos datos modificados al receptor.
  
  Un buffer se basa en respuestas rápidas. Sirve para optimizar el proceso de transferencia de datos entre un software y otro o entre un hardware y un software. Ese dato se guarda en el almacenamiento buffer para que, mientras se realiza el 
  proceso de conexión, éste no se vea afectado. Los buffer suelen ser implementados por software (aunque también puede ser físico) y pueden ser utilizados para todo tipo de sistemas digitales, como vídeos, música, etc.
  No hay que confundir el buffer con la caché del ordenador, aunque ambas cosas tienen un funcionamiento parecido. La diferencia principal es que la caché opera con la premisa de que los datos serán utilizados muchas veces mientras el buffer 
  sirve para un almacenamiento temporal de los datos, es decir, que almacena la información pero sin mantenerla más del tiempo que tarda ésta en enviarse.
  Su función principal es paliar la diferencia de velocidad de transmisión o procesamiento entre dos dispositivos o procesos, y por eso se encuentran por todas partes en un ordenador: disco duro, RAM, impresora, procesador, etc.
  
  Los buffer son datos crudos en hexadecimal optimizados para ser transportados de un punto de memoria a otro. Formalmente, se conocen como streams de datos binarios, pero almacenan los datos en hexadecimal, por lo que cada espacio tiene dos 
  dígitos. Las ventajas de trabajar con buffer radica en que nos permite trabajar con los datos en crudo y por ello quien envía los datos conoce de entrada el tipo de datos y quien recibe los datos también conoce el tipo de datos que contiene
  
  
  
  
  
  